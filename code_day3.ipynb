{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehi1150I7fxN"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hH-OAGszyef5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_hilstrom = 'https://drive.google.com/open?id=15osyN4c5z1pSo1JkxwL_N8bZTksRvQuU'\n",
    "fluff, id = link_hilstrom.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('Hillstrom.csv')\n",
    "hillstrom_df = pd.read_csv('Hillstrom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tAS92JMPe9U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_lalonde = 'https://drive.google.com/open?id=1b8N7WtwIe2WmQJD1KL5UAy70K13MxwKj'\n",
    "fluff, id = link_lalonde.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('Lalonde.csv')\n",
    "lalonde_df = pd.read_csv('Lalonde.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoNrZI5P80wJ"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "def preprocess_data(df, dataset='hillstrom', verbose=True):\n",
    "    # For Hillstrom dataset, the ‘‘visit’’ target variable was selected\n",
    "    #   as the target variable of interest and the selected treatment is \n",
    "    #   the e-mail campaign for women’s merchandise [1]\n",
    "    # [1] Kane K, Lo VSY, Zheng J. True-lift modeling: Comparison of methods. \n",
    "    #    J Market Anal. 2014;2:218–238\n",
    "    dataset = dataset.lower()\n",
    "    if dataset in ('hillstrom', 'email'):\n",
    "        columns = df.columns\n",
    "        for col in columns:\n",
    "            if df[col].dtype != object:\n",
    "                continue\n",
    "            df = pd.concat(\n",
    "                    [df, pd.get_dummies(df[col], \n",
    "                                        prefix=col, \n",
    "                                        drop_first=False)],\n",
    "                    axis=1)\n",
    "            df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        df.columns = [col.replace('-', '').replace(' ', '_').lower()\n",
    "                      for col in df.columns]\n",
    "        df = df[df.segment_mens_email == 0]\n",
    "        df.index = range(len(df))\n",
    "        df.drop(['segment_mens_email', \n",
    "                 'segment_no_email', \n",
    "                 'conversion', \n",
    "                 'spend'], axis=1, inplace=True)\n",
    "\n",
    "        y_name = 'visit'\n",
    "        t_name = 'segment_womens_email'\n",
    "    elif dataset in ['criteo', 'ad']:\n",
    "        raise NotImplementedError\n",
    "    elif dataset in ['lalonde', 'job']:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    df['Y'] = df[y_name]\n",
    "    df.drop([y_name], axis=1, inplace=True)\n",
    "    df['T'] = df[t_name]\n",
    "    df.drop([t_name], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Igf3QLgdJ1cW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def performance(pr_y1_t1, pr_y1_t0, y, t, groups=10):\n",
    "    \"\"\"\n",
    "    1. Split the total customers into the given number of groups\n",
    "    2. Calculate the statistics of each segment\n",
    "    \n",
    "    Args:\n",
    "        pr_y1_t1: the series (list) of the customer's expected return\n",
    "        pr_y1_t0: the expected return when a customer is not treated\n",
    "        y: the observed return of customers\n",
    "        t: whther each customer is treated or not\n",
    "        groups: the number of groups (segments). Should be 5, 10, or 20\n",
    "    Return:\n",
    "        DataFrame:\n",
    "            columns:\n",
    "                'n_y1_t1': the number of treated responders\n",
    "                'n_y1_t0': the number of not treated responders\n",
    "                'r_y1_t1': the average return of treated customers\n",
    "                'r_y1_t0': the average return of not treated customers\n",
    "                'n_t1': the number of treated customers\n",
    "                'n_t0': the number of not treated customers\n",
    "                'uplift': the average uplift (the average treatment effect)\n",
    "            rows: the index of groups\n",
    "    \"\"\"\n",
    "  \n",
    "    ### check valid arguments\n",
    "    if groups not in [5, 10, 20]:\n",
    "        raise Exception(\"uplift: groups must be either 5, 10 or 20\")\n",
    "  \n",
    "    ### check for NAs.\n",
    "    if pr_y1_t1.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in pr_y1_t1\")\n",
    "    if pr_y1_t0.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in pr_y1_t0\")\n",
    "    if y.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in y\")\n",
    "    if t.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in t\")\n",
    "   \n",
    "    ### check valid values for y and t\n",
    "    # if set(y) != {0, 1}:\n",
    "    #     raise Exception(\"uplift: y must be either 0 or 1\")\n",
    "    if set(t) != {0, 1}:\n",
    "        raise Exception(\"uplift: t must be either 0 or 1\")\n",
    "\n",
    "    ### check length of arguments\n",
    "    if not (len(pr_y1_t1) == len(pr_y1_t0) == len(y) == len(t)):\n",
    "        raise Exception(\"uplift: arguments pr_y1_t1, pr_y1_t0, y and t must all have the same length\")\n",
    "\n",
    "    ### define dif_pred\n",
    "    dif_pred = pr_y1_t1 - pr_y1_t0\n",
    "  \n",
    "    ### Make index same\n",
    "    y.index = dif_pred.index\n",
    "    t.index = dif_pred.index\n",
    "    \n",
    "    mm = pd.DataFrame({\n",
    "        'dif_pred': dif_pred,\n",
    "        'y': y,\n",
    "        't': t,\n",
    "        'dif_pred_r': dif_pred.rank(ascending=False, method='first')\n",
    "    })\n",
    "\n",
    "    mm_groupby = mm.groupby(pd.qcut(mm['dif_pred_r'], groups, labels=range(1, groups+1), duplicates='drop'))\n",
    "  \n",
    "    n_y1_t1 = mm_groupby.apply(lambda r: r[r['t'] == 1]['y'].sum())\n",
    "    n_y1_t0 = mm_groupby.apply(lambda r: r[r['t'] == 0]['y'].sum())\n",
    "    n_t1 = mm_groupby['t'].sum()\n",
    "    n_t0 = mm_groupby['t'].count() - n_t1\n",
    "  \n",
    "    df = pd.DataFrame({\n",
    "        'n_t1': n_t1,\n",
    "        'n_t0': n_t0,\n",
    "        'n_y1_t1': n_y1_t1,\n",
    "        'n_y1_t0': n_y1_t0,\n",
    "        'r_y1_t1': n_y1_t1 / n_t1,\n",
    "        'r_y1_t0': n_y1_t0 / n_t0,\n",
    "    })\n",
    "    fillna_columns = ['n_y1_t1', 'n_y1_t0', 'n_t1', 'n_t0']\n",
    "    df[fillna_columns] = df[fillna_columns].fillna(0)\n",
    "    df.index.name = 'groups'\n",
    "\n",
    "    df['uplift'] = df['r_y1_t1'] - df['r_y1_t0']\n",
    "    df['uplift'] = round(df['uplift'], 6)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def qini(perf, plotit=True):\n",
    "    nrow = len(perf)\n",
    "\n",
    "    # Calculating the incremental gains. \n",
    "    # - First, the cumulitative sum of the treated and the control groups are\n",
    "    #  calculated with respect to the total population in each group at the\n",
    "    #  specified decile\n",
    "    # - Afterwards we calculate the percentage of the total amount of people\n",
    "    #  (both treatment and control) are present in each decile\n",
    "    cumul_y1_t1 = (perf['n_y1_t1'].cumsum() / perf['n_t1'].cumsum()).fillna(0)\n",
    "    cumul_y1_t0 = (perf['n_y1_t0'].cumsum() / perf['n_t0'].cumsum()).fillna(0)\n",
    "    deciles = [i/nrow for i in range(1, nrow+1)]\n",
    "\n",
    "    ### Model Incremental gains\n",
    "    inc_gains = (cumul_y1_t1 - cumul_y1_t0) * deciles\n",
    "    inc_gains = [0.0] + list(inc_gains)\n",
    "\n",
    "    ### Overall incremental gains\n",
    "    overall_inc_gain = sum(perf['n_y1_t1']) / sum(perf['n_t1']) \\\n",
    "            - sum(perf['n_y1_t0']) / sum(perf['n_t0'])\n",
    "\n",
    "    ### Random incremental gains\n",
    "    random_inc_gains = [i*overall_inc_gain / nrow for i in range(nrow+1)]\n",
    "\n",
    "    ### Compute area under the model incremental gains (uplift) curve\n",
    "    x = [0] + deciles\n",
    "    y = list(inc_gains)\n",
    "    auuc = 0\n",
    "    auuc_rand = 0\n",
    "\n",
    "    auuc_list = [auuc]\n",
    "    for i in range(1, len(x)):\n",
    "        auuc += 0.5 * (x[i] - x[i-1]) * (y[i] + y[i-1])\n",
    "        auuc_list.append(auuc)\n",
    "\n",
    "    ### Compute area under the random incremental gains curve\n",
    "    y_rand = random_inc_gains\n",
    "\n",
    "    auuc_rand_list = [auuc_rand]\n",
    "    for i in range(1, len(x)):\n",
    "        auuc_rand += 0.5 * (x[i] - x[i-1]) * (y_rand[i] + y_rand[i-1])\n",
    "        auuc_rand_list.append(auuc_rand)\n",
    "\n",
    "    ### Compute the difference between the areas (Qini coefficient)\n",
    "    Qini = auuc - auuc_rand\n",
    "\n",
    "    ### Plot incremental gains curve\n",
    "    if plotit:\n",
    "        x_axis = x\n",
    "        plt.plot(x_axis, inc_gains)\n",
    "        plt.plot(x_axis, random_inc_gains)\n",
    "        plt.show()\n",
    "    \n",
    "    ### Qini 30%, Qini 10%\n",
    "    n_30p = int(nrow*3/10)\n",
    "    n_10p = int(nrow/10)\n",
    "    qini_30p = auuc_list[n_30p] - auuc_rand_list[n_30p]\n",
    "    qini_10p = auuc_list[n_10p] - auuc_rand_list[n_10p]\n",
    "\n",
    "    res = {\n",
    "        'qini': Qini,\n",
    "        'inc_gains': inc_gains,\n",
    "        'random_inc_gains': random_inc_gains,\n",
    "        'auuc_list': auuc_list,\n",
    "        'auuc_rand_list': auuc_rand_list,\n",
    "        'qini_30p': qini_30p,\n",
    "        'qini_10p': qini_10p,\n",
    "    }    \n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nrxjl1v7J9Mm"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def parameter_tuning(fit_mdl, pred_mdl, data, search_space):\n",
    "    \"\"\"\n",
    "    Given a model, search all combination of parameter sets and find\n",
    "    the best parameter set\n",
    "    \n",
    "    Args:\n",
    "        fit_mdl: model function\n",
    "        pred_mdl: predict function of fit_mdl\n",
    "        data:\n",
    "            {\n",
    "                \"x_train\": predictor variables of training dataset,\n",
    "                \"y_train\": target variables of training dataset,\n",
    "                \"t_train\": treatment variables of training dataset,\n",
    "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
    "                \"y_test\": target variables of test (usually, validation) dataset,\n",
    "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
    "            }\n",
    "        search_space:\n",
    "            {\n",
    "                parameter_name: [search values]\n",
    "            }\n",
    "    Return:\n",
    "        The best parameter set\n",
    "    \"\"\"\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    t_train = data['t_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "    t_test = data['t_test']\n",
    "    \n",
    "    max_q = -float('inf')\n",
    "    best_mdl = None\n",
    "\n",
    "    keys = search_space.keys()\n",
    "    n_space = [len(search_space[key]) for key in keys]\n",
    "    n_iter = np.prod(n_space)\n",
    "    \n",
    "    best_params = None\n",
    "    for i in range(n_iter):\n",
    "        params = {}\n",
    "        for idx, key in enumerate(keys):\n",
    "            params[key] = search_space[key][i % n_space[idx]]\n",
    "            i = int(i / n_space[idx])\n",
    "\n",
    "        mdl = fit_mdl(x_train, y_train, t_train, **params)\n",
    "        pred = pred_mdl(mdl, newdata=x_test, y=y_test, ct=t_test)\n",
    "        # print('    {}'.format(params))\n",
    "        try:\n",
    "            perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        q = qini(perf, plotit=False)['qini']\n",
    "        if q > max_q:\n",
    "            max_q = q\n",
    "            best_mdl = mdl\n",
    "            best_params = params\n",
    "\n",
    "    return best_mdl, best_params\n",
    "\n",
    "\n",
    "def wrapper(fit_mdl, pred_mdl, data, params=None,\n",
    "            best_models=None, drop_variables=None, qini_values=None):\n",
    "    \"\"\"\n",
    "    General wrapper approach\n",
    "    \n",
    "    Args:\n",
    "        fit_mdl: model function\n",
    "        pred_mdl: predict function of fit_mdl\n",
    "        data:\n",
    "            {\n",
    "                \"x_train\": predictor variables of training dataset,\n",
    "                \"y_train\": target variables of training dataset,\n",
    "                \"t_train\": treatment variables of training dataset,\n",
    "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
    "                \"y_test\": target variables of test (usually, validation) dataset,\n",
    "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
    "            }\n",
    "    Return:\n",
    "        (A list of best models, The list of dropped variables)\n",
    "    \"\"\"\n",
    "    if best_models is None:\n",
    "        best_models = []\n",
    "    if drop_variables is None:\n",
    "        drop_variables = []\n",
    "    if qini_values is None:\n",
    "        qini_values = []\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    t_train = data['t_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "    t_test = data['t_test']\n",
    "\n",
    "    variables = data['x_train'].columns\n",
    "\n",
    "    max_q = -float('inf')\n",
    "    drop_var = None\n",
    "    best_mdl = None\n",
    "    for var in variables:\n",
    "        if var in drop_variables:\n",
    "            continue\n",
    "        x = x_train.copy()\n",
    "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
    "        mdl = fit_mdl(x, y_train, t_train, **params)\n",
    "        x = x_test.copy()\n",
    "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
    "        pred = pred_mdl(mdl, newdata=x, y=y_test, ct=t_test)\n",
    "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
    "        q = qini(perf, plotit=False)['qini']\n",
    "        if q > max_q:\n",
    "            max_q = q\n",
    "            drop_var = var\n",
    "            best_mdl = mdl\n",
    "    \n",
    "    best_models.append(best_mdl)\n",
    "    drop_variables.append(drop_var)\n",
    "    qini_values.append(max_q)\n",
    "\n",
    "    left_vars = [var for var in variables if (var not in drop_variables)]\n",
    "    \n",
    "    if len(variables) == len(drop_variables) + 1:\n",
    "        return best_models, drop_variables + left_vars, qini_values\n",
    "    else:\n",
    "        return wrapper(fit_mdl, pred_mdl, data, params=params,\n",
    "                       best_models=best_models, drop_variables=drop_variables,\n",
    "                       qini_values=qini_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxrEbEODlbQi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def tma(x, y, t, method=LogisticRegression, **kwargs):\n",
    "    \"\"\"Training a model according to the \"Two Model Approach\" \n",
    "    (a.k.a. \"Separate Model Approach\")\n",
    "    The default model is General Linear Model (GLM)\n",
    "    \n",
    "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
    "\n",
    "    Args:\n",
    "        x: A data frame of predictors.\n",
    "        y: A binary response (numeric) vector.\n",
    "        t: A binary response (numeric) representing the treatment assignment\n",
    "            (coded as 0/1).\n",
    "        method: A sklearn model specifying which classification or regression\n",
    "            model to use. This should be a method that can handle a \n",
    "            multinominal class variable.\n",
    "\n",
    "    Return:\n",
    "        Dictionary: A dictionary of two models. One for the treatment group, \n",
    "            one for the control group.\n",
    "\n",
    "            {\n",
    "                'model_treat': a model for the treatment group,\n",
    "                'model_control': a model for the control group\n",
    "            }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    treat_rows = (t == 1)\n",
    "    control_rows = (t == 0)\n",
    "    model_treat = method(**kwargs).fit(x[treat_rows], y[treat_rows])\n",
    "    model_control = method(**kwargs).fit(x[control_rows], y[control_rows])\n",
    "    \n",
    "    res = {\n",
    "        'model_treat': model_treat,\n",
    "        'model_control': model_control,\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "def predict_tma(obj, newdata, **kwargs):\n",
    "    \"\"\"Predictions according to the \"Two Model Approach\" \n",
    "    (a.k.a. \"Separate Model Approach\")\n",
    "    \n",
    "    For each instance in newdata two predictions are made:\n",
    "    1) What is the probability of a person responding when treated?\n",
    "    2) What is the probability of a person responding when not treated\n",
    "      (i.e. part of control group)?\n",
    "\n",
    "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
    "\n",
    "    Args:\n",
    "        obj: A dictionary of two models. \n",
    "            One for the treatment group, one for the control group.\n",
    "        newdata: A data frame containing the values at which predictions\n",
    "            are required.\n",
    "    \n",
    "    Return:\n",
    "        DataFrame: A dataframe with predicted returns for when the customers\n",
    "            are treated and for when they are not treated.\n",
    "    \"\"\"\n",
    "   \n",
    "    if isinstance(obj['model_treat'], LinearRegression):\n",
    "        pred_treat = obj['model_treat'].predict(newdata)\n",
    "    else:\n",
    "        pred_treat = obj['model_treat'].predict_proba(newdata)[:, 1]\n",
    "\n",
    "    if isinstance(obj['model_control'], LinearRegression):\n",
    "        pred_control = obj['model_control'].predict(newdata)\n",
    "    else:\n",
    "        pred_control = obj['model_control'].predict_proba(newdata)[:, 1]\n",
    "    \n",
    "    # pred_treat = obj['model_treat'].predict(newdata)\n",
    "    # pred_control = obj['model_control'].predict(newdata)\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"pr_y1_t1\": pred_treat,\n",
    "        \"pr_y1_t0\": pred_control,\n",
    "    })\n",
    "    return pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hREl_CRv9DYC"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def ty_assign(y, t):\n",
    "    if y == 1 and t == 1:\n",
    "        return \"TR\"\n",
    "    elif y == 0 and t == 1:\n",
    "        return \"TN\"\n",
    "    elif y == 1 and t == 0:\n",
    "        return \"CR\"\n",
    "    elif y == 0 and t == 0:\n",
    "        return \"CN\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def t_assign(ty):\n",
    "    if ty in (\"TR\", \"TN\"):\n",
    "        return 1\n",
    "    elif ty in (\"CR\", \"CN\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def y_assign(ty):\n",
    "    if ty in (\"TR\", \"CR\"):\n",
    "        return 1\n",
    "    elif ty in (\"TN\", \"CN\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "search_space = {\n",
    "    'method': [LogisticRegression],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'penalty': ['none', 'l2'],\n",
    "    'tol': [1e-2, 1e-3, 1e-4],\n",
    "    'C': [1e6, 1e3, 1, 1e-3, 1e-6],\n",
    "}\n",
    "\n",
    "def main():\n",
    "    ### Load data ###\n",
    "    df = pd.read_csv('Hillstrom.csv')\n",
    "    dataset = 'hillstrom'\n",
    "    df = preprocess_data(df)\n",
    "    Y = df['Y']\n",
    "    T = df['T']\n",
    "    X = df.drop(['Y', 'T'], axis=1)\n",
    "    ty = pd.DataFrame({'Y': Y, 'T': T})\\\n",
    "             .apply(lambda row: ty_assign(row['Y'], row['T']), axis=1)\n",
    "    if dataset == 'hillstrom':\n",
    "        fold_gen = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234).split(X, ty)\n",
    "    elif dataset == 'lalonde':\n",
    "        fold_gen = KFold(n_splits=5, shuffle=True, random_state=1234).split(X)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    ### Cross validation ###\n",
    "    qini_list = []\n",
    "    for idx, (train_index, test_index) in enumerate(fold_gen):\n",
    "        X_train = X.reindex(train_index)\n",
    "        X_test = X.reindex(test_index)\n",
    "        Y_train = Y.reindex(train_index)\n",
    "        Y_test = Y.reindex(test_index)\n",
    "        T_train = T.reindex(train_index)\n",
    "        T_test = T.reindex(test_index)\n",
    "\n",
    "        df = X_train.copy()\n",
    "        df['Y'] = Y_train\n",
    "        df['T'] = T_train\n",
    "        stratify = T_train\n",
    "        if dataset == 'hillstrom':\n",
    "            stratify = df[['Y', 'T']]\n",
    "        tuning_df, validate_df = train_test_split(\n",
    "            df, test_size=0.33, random_state=1234, stratify=stratify)\n",
    "\n",
    "        X_tuning = tuning_df.drop(['Y', 'T'], axis=1)\n",
    "        Y_tuning = tuning_df['Y']\n",
    "        T_tuning = tuning_df['T']\n",
    "\n",
    "        X_validate = validate_df.drop(['Y', 'T'], axis=1)\n",
    "        Y_validate = validate_df['Y']\n",
    "        T_validate = validate_df['T']\n",
    "        \n",
    "        data_dict = {\n",
    "            \"x_train\": X_tuning,\n",
    "            \"y_train\": Y_tuning,\n",
    "            \"t_train\": T_tuning,\n",
    "            \"x_test\": X_validate,\n",
    "            \"y_test\": Y_validate,\n",
    "            \"t_test\": T_validate,\n",
    "        }\n",
    "        \n",
    "        model_method = search_space.get('method', None)\n",
    "        params = {\n",
    "            'method': None if model_method is None else model_method[0],\n",
    "        }\n",
    "        if params['method'] == LogisticRegression:\n",
    "            solver = search_space.get('solver', None)\n",
    "            params['solver'] = None if solver is None else solver[0]\n",
    "\n",
    "        _, drop_vars, qini_values = wrapper(\n",
    "                tma, predict_tma, data_dict, params=params)\n",
    "        best_qini = max(qini_values)\n",
    "        best_idx = qini_values.index(best_qini)\n",
    "        best_drop_vars = drop_vars[:best_idx]\n",
    "\n",
    "        X_tuning.drop(best_drop_vars, axis=1, inplace=True)\n",
    "        X_validate.drop(best_drop_vars, axis=1, inplace=True)\n",
    "        X_train.drop(best_drop_vars, axis=1, inplace=True)\n",
    "        X_test.drop(best_drop_vars, axis=1, inplace=True)\n",
    "\n",
    "        _, best_params = parameter_tuning(tma, predict_tma, data_dict, \n",
    "                                          search_space=search_space)\n",
    "\n",
    "        mdl = tma(X_train, Y_train, T_train, **best_params)\n",
    "        pred = predict_tma(mdl, X_test)\n",
    "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], Y_test, T_test)\n",
    "        q = qini(perf)\n",
    "        qini_list.append(q['qini'])\n",
    "    print('Qini values: ', qini_list)\n",
    "    print('    mean: {}, std: {}'.format(np.mean(qini_list), np.std(qini_list)))\n",
    "        \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "o9X42GUaj-UX",
    "outputId": "efc9f76c-d52b-4f6c-914e-a4df4a21cc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newbie 0\n",
      "recency 3\n",
      "zip_code_rural 0\n",
      "channel_phone 0\n",
      "recency 2\n",
      "womens 0\n",
      "recency 1\n",
      "history_segment_3)_$200__$350 0\n",
      "channel_multichannel 0\n",
      "history_segment_2)_$100__$200 0\n",
      "zip_code_surburban 0\n",
      "channel_multichannel 0\n",
      "channel_multichannel 0\n",
      "zip_code_surburban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_3)_$200__$350 0\n",
      "T 0\n",
      "channel_multichannel 0\n",
      "history_segment_4)_$350__$500 0\n",
      "mens 0\n",
      "history_segment_2)_$100__$200 0\n",
      "zip_code_surburban 0\n",
      "zip_code_surburban 0\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 1\n",
      "mens 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_3)_$200__$350 0\n",
      "mens 0\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_4)_$350__$500 0\n",
      "channel_multichannel 0\n",
      "T 0\n",
      "history_segment_1)_$0__$100 0\n",
      "zip_code_surburban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "mens 0\n",
      "T 0\n",
      "zip_code_surburban 0\n",
      "T 0\n",
      "history_segment_3)_$200__$350 0\n",
      "recency 1\n",
      "mens 0\n",
      "history_segment_2)_$100__$200 0\n",
      "zip_code_surburban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 2\n",
      "zip_code_surburban 0\n",
      "history_segment_2)_$100__$200 0\n",
      "mens 0\n",
      "zip_code_surburban 0\n",
      "mens 0\n",
      "recency 2\n",
      "recency 1\n",
      "womens 0\n",
      "womens 0\n",
      "zip_code_surburban 0\n",
      "history_segment_3)_$200__$350 0\n",
      "recency 1\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 1\n",
      "history_segment_3)_$200__$350 0\n",
      "history_segment_1)_$0__$100 0\n",
      "zip_code_surburban 0\n",
      "recency 2\n",
      "recency 1\n",
      "recency 2\n",
      "mens 0\n",
      "history_segment_4)_$350__$500 0\n",
      "zip_code_surburban 0\n",
      "channel_web 0\n",
      "T 0\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_4)_$350__$500 0\n",
      "channel_multichannel 0\n",
      "mens 0\n",
      "womens 0\n",
      "recency 2\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 2\n",
      "T 0\n",
      "womens 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_2)_$100__$200 0\n",
      "T 0\n",
      "recency 8\n",
      "zip_code_rural 0\n",
      "history_segment_1)_$0__$100 0\n",
      "mens 0\n",
      "zip_code_surburban 0\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 7\n",
      "recency 6\n",
      "channel_phone 0\n",
      "recency 4\n",
      "channel_phone 0\n",
      "recency 4\n",
      "recency 6\n",
      "recency 6\n",
      "zip_code_surburban 0\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 7\n",
      "recency 6\n",
      "womens 0\n",
      "recency 5\n",
      "recency 4\n",
      "recency 5\n",
      "recency 4\n",
      "channel_multichannel 0\n",
      "channel_multichannel 0\n",
      "channel_phone 0\n",
      "recency 5\n",
      "womens 0\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 4\n",
      "mens 0\n",
      "recency 5\n",
      "mens 0\n",
      "mens 0\n",
      "recency 6\n",
      "zip_code_surburban 0\n",
      "zip_code_surburban 0\n",
      "recency 7\n",
      "recency 6\n",
      "channel_web 0\n",
      "mens 0\n",
      "history_segment_4)_$350__$500 0\n",
      "womens 0\n",
      "history_segment_3)_$200__$350 0\n",
      "history_segment_3)_$200__$350 0\n",
      "recency 4\n",
      "recency 5\n",
      "history_segment_1)_$0__$100 0\n",
      "channel_web 0\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 9\n",
      "womens 0\n",
      "zip_code_surburban 0\n",
      "history_segment_2)_$100__$200 0\n",
      "zip_code_rural 0\n",
      "channel_multichannel 0\n",
      "womens 0\n",
      "mens 0\n",
      "recency 11\n",
      "zip_code_surburban 0\n",
      "mens 0\n",
      "recency 9\n",
      "recency 9\n",
      "history_segment_3)_$200__$350 0\n",
      "zip_code_urban 0\n",
      "zip_code_rural 0\n",
      "womens 0\n",
      "recency 10\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_3)_$200__$350 0\n",
      "zip_code_surburban 0\n",
      "channel_phone 0\n",
      "recency 9\n",
      "mens 0\n",
      "recency 10\n",
      "mens 0\n",
      "zip_code_rural 0\n",
      "zip_code_surburban 0\n",
      "zip_code_urban 0\n",
      "mens 0\n",
      "recency 11\n",
      "recency 9\n",
      "zip_code_surburban 0\n",
      "recency 10\n",
      "zip_code_surburban 0\n",
      "zip_code_surburban 0\n",
      "womens 0\n",
      "zip_code_rural 0\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 8\n",
      "recency 7\n",
      "channel_phone 0\n",
      "zip_code_surburban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 5\n",
      "channel_multichannel 0\n",
      "recency 4\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 6\n",
      "zip_code_surburban 0\n",
      "recency 4\n",
      "history_segment_3)_$200__$350 0\n",
      "recency 6\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_2)_$100__$200 0\n",
      "channel_web 0\n",
      "channel_phone 0\n",
      "recency 11\n",
      "recency 9\n",
      "history_segment_2)_$100__$200 0\n",
      "zip_code_surburban 0\n",
      "history_segment_3)_$200__$350 0\n",
      "zip_code_surburban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 10\n",
      "recency 10\n",
      "zip_code_surburban 0\n",
      "recency 9\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 9\n",
      "zip_code_surburban 0\n",
      "channel_phone 0\n",
      "channel_multichannel 0\n",
      "history_segment_3)_$200__$350 0\n",
      "recency 5\n",
      "channel_multichannel 0\n",
      "recency 11\n",
      "recency 9\n",
      "channel_phone 0\n",
      "mens 0\n",
      "history_segment_3)_$200__$350 0\n",
      "channel_web 0\n",
      "recency 9\n",
      "zip_code_surburban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 8\n",
      "recency 7\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 7\n",
      "recency 6\n",
      "zip_code_rural 0\n",
      "recency 6\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 5\n",
      "recency 7\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_4)_$350__$500 0\n",
      "zip_code_rural 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 10\n",
      "recency 10\n",
      "recency 4\n",
      "recency 5\n",
      "zip_code_rural 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 7\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 8\n",
      "recency 9\n",
      "recency 7\n",
      "zip_code_surburban 0\n",
      "recency 8\n",
      "zip_code_surburban 0\n",
      "history_segment_4)_$350__$500 0\n",
      "zip_code_urban 0\n",
      "recency 10\n",
      "recency 8\n",
      "recency 5\n",
      "channel_multichannel 0\n",
      "zip_code_urban 0\n",
      "zip_code_urban 0\n",
      "channel_multichannel 0\n",
      "recency 10\n",
      "recency 5\n",
      "zip_code_urban 0\n",
      "channel_multichannel 0\n",
      "zip_code_rural 0\n",
      "channel_phone 0\n",
      "recency 5\n",
      "womens 0\n",
      "channel_web 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "zip_code_rural 0\n",
      "recency 3\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_6)_$750__$1,000 0\n",
      "channel_multichannel 0\n",
      "history_segment_3)_$200__$350 0\n",
      "T 0\n",
      "recency 2\n",
      "recency 1\n",
      "zip_code_surburban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_1)_$0__$100 0\n",
      "T 0\n",
      "T 0\n",
      "history_segment_4)_$350__$500 0\n",
      "zip_code_surburban 0\n",
      "history_segment_5)_$500__$750 0\n",
      "zip_code_surburban 0\n",
      "recency 1\n",
      "T 0\n",
      "history_segment_5)_$500__$750 0\n",
      "channel_multichannel 0\n",
      "zip_code_surburban 0\n",
      "recency 4\n",
      "recency 4\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 4\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_3)_$200__$350 0\n",
      "T 0\n",
      "recency 3\n",
      "zip_code_urban 0\n",
      "zip_code_rural 0\n",
      "recency 3\n",
      "zip_code_surburban 0\n",
      "recency 1\n",
      "T 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "history_segment_5)_$500__$750 0\n",
      "T 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 1\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "T 0\n",
      "T 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_3)_$200__$350 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_6)_$750__$1,000 0\n",
      "recency 2\n",
      "history_segment_6)_$750__$1,000 0\n",
      "T 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 4\n",
      "recency 4\n",
      "history_segment_7)_$1,000_+ 0\n",
      "zip_code_surburban 0\n",
      "T 0\n",
      "recency 4\n",
      "recency 3\n",
      "recency 4\n",
      "mens 0\n",
      "T 0\n",
      "channel_multichannel 0\n",
      "recency 1\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_5)_$500__$750 0\n",
      "zip_code_urban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "channel_phone 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "history_segment_2)_$100__$200 0\n",
      "zip_code_urban 0\n",
      "channel_phone 0\n",
      "history_segment_5)_$500__$750 0\n",
      "recency 3\n",
      "recency 2\n",
      "zip_code_rural 0\n",
      "zip_code_rural 0\n",
      "history_segment_6)_$750__$1,000 0\n",
      "recency 4\n",
      "zip_code_rural 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_6)_$750__$1,000 0\n",
      "recency 2\n",
      "recency 4\n",
      "recency 3\n",
      "history_segment_5)_$500__$750 0\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 2\n",
      "channel_phone 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 4\n",
      "history_segment_3)_$200__$350 0\n",
      "zip_code_urban 0\n",
      "recency 2\n",
      "history_segment_6)_$750__$1,000 0\n",
      "recency 1\n",
      "channel_multichannel 0\n",
      "history_segment_2)_$100__$200 0\n",
      "channel_phone 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_3)_$200__$350 0\n",
      "channel_web 0\n",
      "recency 3\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 2\n",
      "zip_code_urban 0\n",
      "history_segment_5)_$500__$750 0\n",
      "zip_code_surburban 0\n",
      "zip_code_surburban 0\n",
      "recency 4\n",
      "recency 4\n",
      "recency 2\n",
      "history_segment_1)_$0__$100 0\n",
      "zip_code_urban 0\n",
      "history_segment_5)_$500__$750 0\n",
      "recency 4\n",
      "recency 3\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 3\n",
      "channel_web 0\n",
      "channel_web 0\n",
      "T 0\n",
      "zip_code_rural 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "recency 4\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 3\n",
      "recency 1\n",
      "channel_multichannel 0\n",
      "zip_code_surburban 0\n",
      "channel_multichannel 0\n",
      "zip_code_surburban 0\n",
      "channel_web 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_3)_$200__$350 0\n",
      "zip_code_urban 0\n",
      "channel_multichannel 0\n",
      "zip_code_rural 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "recency 4\n",
      "recency 2\n",
      "channel_phone 0\n",
      "history_segment_5)_$500__$750 0\n",
      "recency 2\n",
      "channel_web 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "recency 1\n",
      "channel_phone 0\n",
      "T 0\n",
      "channel_phone 0\n",
      "history_segment_5)_$500__$750 0\n",
      "history_segment_6)_$750__$1,000 0\n",
      "zip_code_rural 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "channel_multichannel 0\n",
      "recency 9\n",
      "recency 8\n",
      "zip_code_surburban 0\n",
      "history_segment_1)_$0__$100 0\n",
      "womens 0\n",
      "mens 0\n",
      "recency 6\n",
      "history_segment_2)_$100__$200 0\n",
      "womens 0\n",
      "zip_code_surburban 0\n",
      "history_segment_3)_$200__$350 0\n",
      "mens 0\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 11\n",
      "history_segment_1)_$0__$100 0\n",
      "zip_code_surburban 0\n",
      "history_segment_4)_$350__$500 0\n",
      "zip_code_surburban 0\n",
      "recency 10\n",
      "recency 10\n",
      "mens 0\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 6\n",
      "history_segment_3)_$200__$350 0\n",
      "mens 0\n",
      "recency 8\n",
      "history_segment_4)_$350__$500 0\n",
      "womens 0\n",
      "history_segment_3)_$200__$350 0\n",
      "zip_code_rural 0\n",
      "recency 7\n",
      "womens 0\n",
      "recency 11\n",
      "zip_code_rural 0\n",
      "history_segment_6)_$750__$1,000 0\n",
      "history_segment_7)_$1,000_+ 0\n",
      "history_segment_3)_$200__$350 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_5)_$500__$750 0\n",
      "recency 8\n",
      "recency 7\n",
      "zip_code_surburban 0\n",
      "mens 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 10\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 9\n",
      "recency 7\n",
      "recency 10\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 10\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 9\n",
      "recency 8\n",
      "womens 0\n",
      "recency 8\n",
      "history_segment_5)_$500__$750 0\n",
      "history_segment_2)_$100__$200 0\n",
      "zip_code_rural 0\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 6\n",
      "history_segment_7)_$1,000_+ 0\n",
      "channel_multichannel 0\n",
      "channel_phone 0\n",
      "zip_code_surburban 0\n",
      "channel_phone 0\n",
      "recency 7\n",
      "recency 6\n",
      "zip_code_rural 0\n",
      "channel_web 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_5)_$500__$750 0\n",
      "history_segment_2)_$100__$200 0\n",
      "zip_code_surburban 0\n",
      "recency 10\n",
      "recency 9\n",
      "zip_code_surburban 0\n",
      "recency 11\n",
      "history_segment_3)_$200__$350 0\n",
      "zip_code_surburban 0\n",
      "recency 9\n",
      "recency 10\n",
      "history_segment_5)_$500__$750 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 9\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_4)_$350__$500 0\n",
      "mens 0\n",
      "channel_web 0\n",
      "recency 11\n",
      "recency 7\n",
      "history_segment_5)_$500__$750 0\n",
      "history_segment_1)_$0__$100 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_2)_$100__$200 0\n",
      "recency 10\n",
      "zip_code_rural 0\n",
      "channel_multichannel 0\n",
      "history_segment_3)_$200__$350 0\n",
      "recency 9\n",
      "history_segment_4)_$350__$500 0\n",
      "recency 8\n",
      "history_segment_5)_$500__$750 0\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_5)_$500__$750 0\n",
      "zip_code_surburban 0\n",
      "history_segment_5)_$500__$750 0\n",
      "history_segment_2)_$100__$200 0\n",
      "history_segment_3)_$200__$350 0\n",
      "history_segment_6)_$750__$1,000 0\n",
      "zip_code_urban 0\n",
      "recency 8\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 7\n",
      "zip_code_rural 0\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 9\n",
      "recency 8\n",
      "recency 7\n",
      "history_segment_1)_$0__$100 0\n",
      "recency 9\n",
      "zip_code_rural 0\n",
      "history_segment_3)_$200__$350 0\n",
      "recency 11\n",
      "recency 7\n",
      "history_segment_6)_$750__$1,000 0\n",
      "pred: 0       0.163265\n",
      "1       0.125000\n",
      "2       0.052632\n",
      "3       0.063830\n",
      "4       0.058824\n",
      "5       0.089888\n",
      "6       0.070423\n",
      "7       0.060606\n",
      "8       0.072289\n",
      "9       0.006536\n",
      "10      0.184211\n",
      "11      0.118421\n",
      "12      0.210526\n",
      "13      0.052632\n",
      "14      0.269841\n",
      "15      0.137931\n",
      "16      0.387097\n",
      "17      0.037037\n",
      "18      0.291667\n",
      "19      0.096774\n",
      "20      0.019608\n",
      "21      0.196429\n",
      "22      0.015152\n",
      "23      0.129870\n",
      "24      0.131313\n",
      "25      0.068493\n",
      "26      0.210526\n",
      "27      0.144578\n",
      "28      0.026316\n",
      "29      0.118644\n",
      "          ...   \n",
      "8506    0.016667\n",
      "8507    0.215909\n",
      "8508    0.032258\n",
      "8509    0.076923\n",
      "8510    0.206349\n",
      "8511    0.016667\n",
      "8512    0.069767\n",
      "8513    0.139535\n",
      "8514    0.175824\n",
      "8515    0.050000\n",
      "8516    0.372881\n",
      "8517    0.372881\n",
      "8518    0.142857\n",
      "8519    0.206186\n",
      "8520    0.060000\n",
      "8521    0.089888\n",
      "8522    0.204082\n",
      "8523    0.182796\n",
      "8524    0.086957\n",
      "8525    0.011299\n",
      "8526    0.127273\n",
      "8527    0.106383\n",
      "8528    0.022222\n",
      "8529    0.206186\n",
      "8530    0.101124\n",
      "8531    0.092593\n",
      "8532    0.125000\n",
      "8533    0.136364\n",
      "8534    0.083333\n",
      "8535    0.071429\n",
      "Length: 8536, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, attribute, threshold):\n",
    "        self.attr = attribute\n",
    "        self.thres = threshold\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf = False\n",
    "        self.predict = None\n",
    "\n",
    "\n",
    "def info_gain(df, attribute, predict_attr):\n",
    "    \"\"\"\n",
    "    Select the information gain and threshold of the attribute to split\n",
    "    The threshold chosen splits the test data such that information gain is maximized\n",
    "    \n",
    "    Return a pandas.DataFrame\n",
    "        columns: 'thres' (threshold) and 'info_gain' (information gain)\n",
    "    \"\"\"\n",
    "    num_total = df.shape[0]\n",
    "    tmp = pd.DataFrame({\n",
    "        'thres': df[attribute],\n",
    "        'Y': df[predict_attr]\n",
    "    })\n",
    "    tmp.sort_values(['thres'], inplace=True)\n",
    "\n",
    "    # Left child node has the observations whose attributes are less than (or equual to) the threshold\n",
    "    # Right child has has the observations whose attributes are greater than the threshold\n",
    "    #   n_pos_L: The number of the positive (y=1) observations in the left child\n",
    "    #   n_neg_L: The number of the negative (y=0) observations in the left child\n",
    "    #   n_L: The number observations in the left child\n",
    "    #   r_pos_L: The average return of the observations in the left child\n",
    "    #   r_pos_R: The average return of the observations in the right child\n",
    "    y_total = sum(df[predict_attr])\n",
    "    tmp['n_pos_L'] = tmp['Y'].cumsum()\n",
    "    tmp['n_neg_L'] = (tmp['Y'] == 0).cumsum()\n",
    "    tmp['n_L'] = tmp['n_pos_L'] + tmp['n_neg_L']\n",
    "    tmp['r_pos_L'] = tmp['n_pos_L']/(tmp['n_L'])\n",
    "    tmp['r_pos_R'] = (y_total - tmp['n_pos_L'])/(num_total - tmp['n_L'])\n",
    "\n",
    "    # H_L: entropy of left child \n",
    "    tmp['H_L'] = -tmp['r_pos_L']*np.log2(tmp['r_pos_L'])\\\n",
    "            -(1-tmp['r_pos_L'])*np.log2(1-tmp['r_pos_L'])\n",
    "    # H_R: entropy of right child\n",
    "    tmp['H_R'] = -tmp['r_pos_R']*np.log2(tmp['r_pos_R'])\\\n",
    "            -(1-tmp['r_pos_R'])*np.log2(1-tmp['r_pos_R'])\n",
    "    # EH: Expected entropy\n",
    "    tmp['EH'] = ((tmp['n_L'])*tmp['H_L'] + \\\n",
    "                 (num_total - tmp['n_L'])*tmp['H_R'])/num_total\n",
    "\n",
    "    # We will select one rows per one distinct candidate\n",
    "    dups = tmp['thres'].duplicated(keep='last')\n",
    "    tmp['thres_ok'] = (dups == False)\n",
    "    tmp.dropna(inplace=True)\n",
    "    if sum(tmp['thres_ok']) < 1:\n",
    "        return None\n",
    "\n",
    "    tmp = tmp[tmp['thres_ok']]\n",
    "    tmp['info_gain'] = info_entropy(df, predict_attr) - tmp['EH']\n",
    "    \n",
    "    return tmp[['thres', 'info_gain']]\n",
    "\n",
    "\n",
    "def info_entropy(df, predict_attr):\n",
    "    \"\"\"\n",
    "    Calculate info content (entropy) of the test data\n",
    "    \"\"\"\n",
    "    # Dataframe and number of positive/negatives examples in the data\n",
    "    p_df = df[df[predict_attr] == 1]\n",
    "    n_df = df[df[predict_attr] == 0]\n",
    "    p = float(p_df.shape[0])\n",
    "    n = float(n_df.shape[0])\n",
    "    if p  == 0 or n == 0:\n",
    "        I = 0\n",
    "    else:\n",
    "        I = ((-1*p)/(p + n))*math.log(p/(p+n), 2) + ((-1*n)/(p + n))*math.log(n/(p+n), 2)\n",
    "    return I    \n",
    "\n",
    "\n",
    "def num_class(df, predict_attr):\n",
    "    \"\"\"\n",
    "    Returns the number of positive and negative data\n",
    "    \"\"\"\n",
    "    p_df = df[df[predict_attr] == 1]\n",
    "    n_df = df[df[predict_attr] == 0]\n",
    "    return p_df.shape[0], n_df.shape[0]\n",
    "\n",
    "\n",
    "def choose_attr(df, attributes, predict_attr):\n",
    "    \"\"\"\n",
    "    Chooses the attribute and its threshold with the highest info gain\n",
    "    from the set of attributes\n",
    "    \"\"\"\n",
    "    max_info_gain = 0\n",
    "    best_attr = None\n",
    "    threshold = None\n",
    "    # Test each attribute (note attributes maybe be chosen more than once)\n",
    "    for attr in attributes:\n",
    "        df_ig = info_gain(df, attr, predict_attr)\n",
    "        if df_ig is None:\n",
    "            continue\n",
    "\n",
    "        # Get the possible indices of maximum info gain\n",
    "        ig = max(df_ig['info_gain'])\n",
    "        idx_ig = df_ig.index[df_ig['info_gain']==ig]\n",
    "        # Break ties randomly\n",
    "        idx_ig = random.choice(idx_ig)\n",
    "        # Get information gain & threshold of that\n",
    "        thres = df_ig['thres'][idx_ig]\n",
    "\n",
    "        if ig > max_info_gain:\n",
    "            max_info_gain = ig\n",
    "            best_attr = attr\n",
    "            threshold = thres\n",
    "    return best_attr, threshold\n",
    "\n",
    "\n",
    "\n",
    "def build_tree(df, cols, predict_attr):\n",
    "    \"\"\"\n",
    "    Builds the Decision Tree based on training data, attributes to train on,\n",
    "    and a prediction attribute\n",
    "    \"\"\"\n",
    "    # Get the number of positive and negative examples in the training data\n",
    "    p, n = num_class(df, predict_attr)\n",
    "    # If train data has all positive or all negative values, or the number of\n",
    "    # the train data is less than the given minimum number of split, then we\n",
    "    # have reached the end of our tree\n",
    "    if p > 0 and n > 0 and (p+n) >= 100:\n",
    "        # Determine attribute and its threshold value with the highest\n",
    "        # information gain\n",
    "        best_attr, threshold = choose_attr(df, cols, predict_attr)\n",
    "        if best_attr is None:\n",
    "            # Create a leaf node indicating it's prediction\n",
    "            leaf = Node(None,None)\n",
    "            leaf.leaf = True\n",
    "            leaf.predict = p / (p+n)\n",
    "            return leaf\n",
    "        # Create internal tree node based on attribute and it's threshold\n",
    "        sub_1 = df[df[best_attr] <= threshold]\n",
    "        sub_2 = df[df[best_attr] > threshold]\n",
    "        if sub_1.shape[0] > 0 and sub_2.shape[0] > 0:\n",
    "            tree = Node(best_attr, threshold)\n",
    "            # Recursively build left and right subtree\n",
    "            tree.left = build_tree(sub_1, cols, predict_attr)\n",
    "            tree.right = build_tree(sub_2, cols, predict_attr)\n",
    "            return tree\n",
    "    # Create a leaf node indicating it's prediction\n",
    "    leaf = Node(None,None)\n",
    "    leaf.leaf = True\n",
    "    leaf.predict = p / (p+n)\n",
    "    return leaf\n",
    "\n",
    "\n",
    "def predict(node, row_df):\n",
    "    \"\"\"\n",
    "    Given a instance of a training data, make a prediction of an observation (row)\n",
    "    based on the Decision Tree\n",
    "    Assumes all data has been cleaned (i.e. no NULL data)\n",
    "    \"\"\"\n",
    "    # If we are at a leaf node, return the prediction of the leaf node\n",
    "    if node.leaf:\n",
    "        return node.predict\n",
    "    # Traverse left or right subtree based on instance's data\n",
    "    if row_df[node.attr] <= node.thres:\n",
    "        return predict(node.left, row_df)\n",
    "    elif row_df[node.attr] > node.thres:\n",
    "        return predict(node.right, row_df)\n",
    "\n",
    "\n",
    "def test_predictions(root, df):\n",
    "    \"\"\"\n",
    "    Given a set of data, make a prediction for each instance using the Decision Tree\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "    for index,row in df.iterrows():\n",
    "        prediction.append(predict(root, row))\n",
    "    pred_df = pd.Series(prediction)\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# Define X_train, Y_train, T_train, X_test, Y_test, and T_test\n",
    "# An example use of 'build_tree' and 'predict'\n",
    "df_train = X_train.copy()\n",
    "df_train['Y'] = Y_train\n",
    "df_train['T'] = T_train\n",
    "df_test = X_test.copy()\n",
    "df_test['Y'] = Y_test\n",
    "df_test['T'] = T_test\n",
    "\n",
    "df_train.drop(['history'], axis=1, inplace=True)\n",
    "df_test.drop(['history'], axis=1, inplace=True)\n",
    "\n",
    "assert((df_train.columns == df_test.columns).all())\n",
    "attributes = [c for c in df_train.columns if c != 'Y']\n",
    "root = build_tree(df_train, attributes, 'Y')\n",
    "pred = test_predictions(root, df_test)\n",
    "print('pred: {}'.format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ORX-_PRzfNw"
   },
   "outputs": [],
   "source": [
    "def uplift_tree_tma(x, y, t, **kwargs):\n",
    "    \n",
    "    df = x.copy()\n",
    "    df['Y'] = y\n",
    "    df['T'] = t\n",
    "    \n",
    "    features = [feat for feat in x.columns]\n",
    "    model_treat = build_tree(df[t==1], features, predict_attr='Y')\n",
    "    model_control = build_tree(df[t==0], features, predict_attr='Y')\n",
    "    \n",
    "    res = {\n",
    "        'model_treat': model_treat,\n",
    "        'model_control': model_control,\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "def predict_tree_tma(obj, newdata, **kwargs):\n",
    "    pred_treat = test_predictions(obj['model_treat'], newdata, **kwargs)\n",
    "    pred_control = test_predictions(obj['model_control'], newdata, **kwargs)\n",
    "    \n",
    "    pred_df = pd.DataFrame({\n",
    "        \"pr_y1_t1\": pred_treat,\n",
    "        \"pr_y1_t0\": pred_control,\n",
    "    })\n",
    "    return pred_df\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nXm5s91SyegK",
    "outputId": "84050076-f2a6-4d72-ce46-b00eb90877b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "womens 0\n",
      "newbie 0\n",
      "zip_code_rural 0\n",
      "recency 3\n",
      "history_segment_3)_$200__$350 0\n",
      "history 411.76\n",
      "history 156.04\n",
      "history 153.51\n",
      "recency 2\n",
      "recency 1\n",
      "history 49.99\n",
      "history 57.44\n",
      "history 73.69\n",
      "history 34.77\n",
      "history 50.13\n",
      "history 90.22\n",
      "history 79.84\n",
      "history 168.83\n",
      "history 169.53\n",
      "channel_phone 0\n",
      "history 426.81\n",
      "channel_web 0\n",
      "history 218.1\n",
      "history 220.43\n",
      "history 335.07\n",
      "history 321.69\n",
      "recency 1\n",
      "recency 1\n",
      "history 331.82\n",
      "recency 8\n",
      "recency 7\n",
      "channel_phone 0\n",
      "zip_code_surburban 0\n",
      "history 316.36\n",
      "history 71.48\n",
      "history_segment_2)_$100__$200 0\n",
      "channel_multichannel 0\n",
      "history 159.71\n",
      "history 154.21\n",
      "recency 6\n",
      "history 78.41\n",
      "zip_code_surburban 0\n",
      "recency 4\n",
      "history 228.88\n",
      "history 135.36\n",
      "history 318.56\n",
      "history_segment_2)_$100__$200 0\n",
      "history 70.18\n",
      "history 173.21\n",
      "history 75.19\n",
      "channel_phone 0\n",
      "recency 9\n",
      "history 127.38\n",
      "recency 11\n",
      "history 63.07\n",
      "history 29.99\n",
      "history 170.69\n",
      "zip_code_surburban 0\n",
      "recency 10\n",
      "zip_code_surburban 0\n",
      "history 289.99\n",
      "history 148.98\n",
      "history 29.99\n",
      "history 174.11\n",
      "history 114.85\n",
      "history 45.29\n",
      "history 44.17\n",
      "channel_phone 0\n",
      "history 332.8\n",
      "history 345.49\n",
      "history 359.67\n",
      "history 379.05\n",
      "history 381.16\n",
      "recency 7\n",
      "recency 9\n",
      "channel_web 0\n",
      "history 486.3\n",
      "recency 2\n",
      "recency 3\n",
      "recency 5\n",
      "recency 10\n",
      "history 135.11\n",
      "history_segment_3)_$200__$350 0\n",
      "history 81.48\n",
      "recency 9\n",
      "recency 8\n",
      "history 668.48\n",
      "channel_web 0\n",
      "history 68.03\n",
      "history 34.89\n",
      "history 33.99\n",
      "recency 4\n",
      "recency 2\n",
      "zip_code_rural 0\n",
      "history 68.29\n",
      "zip_code_rural 0\n",
      "recency 5\n",
      "history 74.87\n",
      "history 122.33\n",
      "history 92.89\n",
      "history 123.73\n",
      "channel_multichannel 0\n",
      "history 532.98\n",
      "recency 2\n",
      "history 158.9\n",
      "history 183.25\n",
      "recency 1\n",
      "history 537.19\n",
      "recency 6\n",
      "history 112.02\n",
      "history 387.96\n",
      "history 82.74\n",
      "history 419.55\n",
      "history 246.66\n",
      "recency 6\n",
      "zip_code_rural 0\n",
      "history 189.96\n",
      "history 188.42\n",
      "history 160.2\n",
      "history 130.47\n",
      "history 130.34\n",
      "recency 5\n",
      "history 52.33\n",
      "history 49.5\n",
      "history 38.79\n",
      "history 37.75\n",
      "history 30.62\n",
      "history 88.51\n",
      "recency 7\n",
      "history 267.7\n",
      "recency 5\n",
      "history_segment_4)_$350__$500 0\n",
      "history_segment_6)_$750__$1,000 0\n",
      "history 669.58\n",
      "recency 7\n",
      "recency 3\n",
      "history 694.52\n",
      "history 1174.57\n",
      "recency 3\n",
      "zip_code_rural 0\n",
      "channel_web 0\n",
      "history 137.96\n",
      "history 46.31\n",
      "history 45.55\n",
      "zip_code_surburban 0\n",
      "recency 11\n",
      "history 173.36\n",
      "history_segment_4)_$350__$500 0\n",
      "history 696.43\n",
      "recency 11\n",
      "history 45.09\n",
      "recency 9\n",
      "history 45.88\n",
      "history 58.89\n",
      "recency 10\n",
      "history_segment_3)_$200__$350 0\n",
      "history 119.29\n",
      "history 145.54\n",
      "history 37.33\n",
      "mens 0\n",
      "newbie 0\n",
      "recency 2\n",
      "history 292.13\n",
      "channel_phone 0\n",
      "history 30.13\n",
      "history 46.05\n",
      "zip_code_urban 0\n",
      "history 148.9\n",
      "history 183.41\n",
      "history 104.15\n",
      "history 70.19\n",
      "history 66.05\n",
      "history 116.54\n",
      "history 251.13\n",
      "history 239.33\n",
      "history 182.91\n",
      "recency 1\n",
      "history 293.43\n",
      "history 480.21\n",
      "history 302.57\n",
      "history 318.91\n",
      "history 366.04\n",
      "history 344.86\n",
      "history_segment_3)_$200__$350 0\n",
      "history 34.84\n",
      "recency 7\n",
      "history 33.82\n",
      "history 29.99\n",
      "zip_code_surburban 0\n",
      "recency 6\n",
      "history 34.31\n",
      "recency 10\n",
      "recency 9\n",
      "history 33.03\n",
      "zip_code_urban 0\n",
      "history 189.78\n",
      "history 118.45\n",
      "history 101.21\n",
      "history 90.29\n",
      "history 88.13\n",
      "recency 6\n",
      "recency 3\n",
      "history 38.76\n",
      "history 42.25\n",
      "history 43.48\n",
      "channel_phone 0\n",
      "history 77.1\n",
      "history 60.69\n",
      "history 57.56\n",
      "history 51.37\n",
      "history 47.15\n",
      "recency 8\n",
      "channel_phone 0\n",
      "recency 4\n",
      "recency 6\n",
      "history 122.95\n",
      "history 128.85\n",
      "history 131.55\n",
      "history 133.13\n",
      "history 139.72\n",
      "history 151.21\n",
      "history 167.33\n",
      "zip_code_urban 0\n",
      "history 170.35\n",
      "recency 4\n",
      "history 187.35\n",
      "history 186.81\n",
      "history 175.33\n",
      "recency 8\n",
      "history 197.7\n",
      "history 368.18\n",
      "recency 5\n",
      "history 370.76\n",
      "zip_code_surburban 0\n",
      "zip_code_surburban 0\n",
      "history 373.38\n",
      "history 338.88\n",
      "recency 10\n",
      "recency 8\n",
      "history 262.77\n",
      "history 236.69\n",
      "history 227.45\n",
      "history 271.03\n",
      "channel_multichannel 0\n",
      "zip_code_rural 0\n",
      "history 334.27\n",
      "history 325.58\n",
      "history 268.71\n",
      "zip_code_urban 0\n",
      "history 647.47\n",
      "recency 4\n",
      "history 607.33\n",
      "history 605.95\n",
      "history 561.82\n",
      "history 547.86\n",
      "history 277.55\n",
      "history 264.03\n",
      "recency 1\n",
      "channel_phone 0\n",
      "zip_code_urban 0\n",
      "history 160.19\n",
      "recency 2\n",
      "history 197.86\n",
      "history 190.02\n",
      "history 52.37\n",
      "history 173.12\n",
      "history 115.81\n",
      "history 63.02\n",
      "history 30.59\n",
      "history 126.74\n",
      "zip_code_urban 0\n",
      "history 217.48\n",
      "history 196.22\n",
      "history 191.17\n",
      "history 161.34\n",
      "history 439.98\n",
      "history 391.46\n",
      "history 303.08\n",
      "history 455.35\n",
      "channel_web 0\n",
      "history_segment_3)_$200__$350 0\n",
      "channel_web 0\n",
      "history 152.78\n",
      "history 111.69\n",
      "zip_code_rural 0\n",
      "recency 11\n",
      "history 107.54\n",
      "recency 9\n",
      "history 53.41\n",
      "history 46.35\n",
      "history 29.99\n",
      "history 88.31\n",
      "history 68.03\n",
      "zip_code_surburban 0\n",
      "history 127.84\n",
      "history 153.87\n",
      "recency 10\n",
      "history 431.23\n",
      "recency 7\n",
      "channel_multichannel 0\n",
      "history 114.7\n",
      "history 73.52\n",
      "history 59.46\n",
      "history 58.87\n",
      "zip_code_urban 0\n",
      "history 47.45\n",
      "history 39.67\n",
      "history 32.44\n",
      "history 31.5\n",
      "recency 7\n",
      "history 43.11\n",
      "history 113.39\n",
      "history 74.18\n",
      "recency 5\n",
      "history 81.6\n",
      "history 92.58\n",
      "history 132.28\n",
      "history 132.61\n",
      "recency 10\n",
      "history_segment_4)_$350__$500 0\n",
      "zip_code_surburban 0\n",
      "recency 8\n",
      "history 259.77\n",
      "history 254.8\n",
      "history 1215.35\n",
      "history 780.12\n",
      "history 648.79\n",
      "channel_web 0\n",
      "recency 11\n",
      "history 791.31\n",
      "history 801.22\n",
      "history 820.36\n",
      "history 1030.78\n",
      "recency 4\n",
      "history 305.15\n",
      "history 242.96\n",
      "history 281.05\n",
      "newbie 0\n",
      "history 448.34\n",
      "history 445.71\n",
      "history 415.14\n",
      "history 398.85\n",
      "channel_phone 0\n",
      "history 375.32\n",
      "history 424.61\n",
      "history 451.15\n",
      "history 505.9\n",
      "history 531.19\n",
      "history 711.84\n",
      "history 702.75\n",
      "recency 1\n",
      "history 745.63\n",
      "history 756.5\n",
      "history 773.32\n",
      "history 842.86\n",
      "history 905.83\n",
      "history 1049.71\n",
      "zip_code_urban 0\n",
      "history 202.7\n",
      "history 787.02\n",
      "history 706.81\n",
      "history 210.51\n",
      "history 214.59\n",
      "recency 11\n",
      "history 416.92\n",
      "history 306.69\n",
      "history 303.37\n",
      "history 295.99\n",
      "history 292.06\n",
      "history 287.25\n",
      "history 286.35\n",
      "recency 7\n",
      "history 271.42\n",
      "history 345.93\n",
      "history 420.67\n",
      "recency 10\n",
      "recency 7\n",
      "history 523.85\n",
      "newbie 0\n",
      "recency 7\n",
      "zip_code_rural 0\n",
      "history 160.58\n",
      "recency 3\n",
      "history 155.89\n",
      "mens 0\n",
      "channel_phone 0\n",
      "history 128.8\n",
      "history 48.42\n",
      "history 71.15\n",
      "history 155.03\n",
      "history 69.65\n",
      "recency 2\n",
      "history 86.74\n",
      "history 93.15\n",
      "history 153.47\n",
      "history 32.45\n",
      "history 30.85\n",
      "channel_phone 0\n",
      "history 74.03\n",
      "history 55.77\n",
      "history 82.76\n",
      "history 95.23\n",
      "history 95.93\n",
      "history 112.49\n",
      "zip_code_surburban 0\n",
      "history 158.91\n",
      "history 151.28\n",
      "history 146.33\n",
      "history 137.25\n",
      "history 136.22\n",
      "history 126.71\n",
      "history 124.41\n",
      "history 114.74\n",
      "history 111.2\n",
      "history 96.51\n",
      "history 85.73\n",
      "history 70.74\n",
      "history 70.3\n",
      "mens 0\n",
      "history 54.77\n",
      "recency 4\n",
      "history 52.69\n",
      "zip_code_surburban 0\n",
      "recency 5\n",
      "history 31.69\n",
      "history 66.6\n",
      "recency 6\n",
      "channel_phone 0\n",
      "recency 2\n",
      "history 366.22\n",
      "history 326.19\n",
      "history 323.98\n",
      "history 311.85\n",
      "history 305.43\n",
      "history 304.07\n",
      "history 292.2\n",
      "history 287.81\n",
      "history 275.86\n",
      "history 272.42\n",
      "history 261.2\n",
      "history 248.87\n",
      "history 218.06\n",
      "history 380.69\n",
      "history 385.23\n",
      "history 387.24\n",
      "history 472.82\n",
      "history 463.35\n",
      "history 423.9\n",
      "history 496.92\n",
      "history 233.55\n",
      "history 215.21\n",
      "history 207.8\n",
      "history 206.05\n",
      "history 190.65\n",
      "history 381.74\n",
      "recency 3\n",
      "channel_multichannel 0\n",
      "recency 4\n",
      "history 364.6\n",
      "history 358.77\n",
      "mens 0\n",
      "history 382.68\n",
      "history 478.31\n",
      "history 461.81\n",
      "history 449.64\n",
      "history 447.44\n",
      "history 398.81\n",
      "history 430.91\n",
      "history 225.36\n",
      "womens 0\n",
      "history 163.06\n",
      "history 166.13\n",
      "history 168.64\n",
      "history 175.95\n",
      "recency 5\n",
      "history 193.99\n",
      "history 226.36\n",
      "history 318.0\n",
      "history 315.69\n",
      "history 260.15\n",
      "history 274.82\n",
      "history 283.69\n",
      "history 289.3\n",
      "history 406.54\n",
      "history 385.69\n",
      "history 380.3\n",
      "zip_code_surburban 0\n",
      "history 412.28\n",
      "recency 1\n",
      "history 442.51\n",
      "recency 4\n",
      "history 179.99\n",
      "history 50.85\n",
      "history 140.87\n",
      "history 97.72\n",
      "history 186.83\n",
      "womens 0\n",
      "history 410.79\n",
      "mens 0\n",
      "recency 1\n",
      "history 440.56\n",
      "history 314.42\n",
      "history 281.32\n",
      "channel_multichannel 0\n",
      "channel_phone 0\n",
      "history 165.98\n",
      "history 82.03\n",
      "history 70.18\n",
      "channel_phone 0\n",
      "history 68.07\n",
      "history 60.87\n",
      "history 57.27\n",
      "history 48.94\n",
      "history 43.51\n",
      "history 29.99\n",
      "zip_code_rural 0\n",
      "recency 10\n",
      "zip_code_surburban 0\n",
      "recency 8\n",
      "recency 9\n",
      "mens 0\n",
      "recency 11\n",
      "zip_code_rural 0\n",
      "recency 10\n",
      "zip_code_surburban 0\n",
      "history 79.65\n",
      "zip_code_rural 0\n",
      "channel_web 0\n",
      "history 321.25\n",
      "history 99.09\n",
      "recency 8\n",
      "recency 10\n",
      "history 115.37\n",
      "history 145.45\n",
      "history 149.18\n",
      "channel_multichannel 0\n",
      "womens 0\n",
      "history 269.33\n",
      "recency 9\n",
      "history 142.66\n",
      "history 144.2\n",
      "recency 11\n",
      "history 370.35\n",
      "history 340.24\n",
      "mens 0\n",
      "history 408.91\n",
      "history_segment_3)_$200__$350 0\n",
      "recency 10\n",
      "history 83.98\n",
      "history 94.08\n",
      "recency 9\n",
      "history 130.29\n",
      "history 166.68\n",
      "history 416.5\n",
      "zip_code_rural 0\n",
      "history 142.43\n",
      "history 194.3\n",
      "history 138.65\n",
      "recency 10\n",
      "history 234.85\n",
      "history 254.62\n",
      "mens 0\n",
      "recency 5\n",
      "history 600.84\n",
      "zip_code_rural 0\n",
      "recency 1\n",
      "channel_web 0\n",
      "history_segment_3)_$200__$350 0\n",
      "history 38.13\n",
      "history 116.33\n",
      "history 117.06\n",
      "womens 0\n",
      "history 507.82\n",
      "history_segment_3)_$200__$350 0\n",
      "history 531.17\n",
      "history 160.89\n",
      "history 73.01\n",
      "history 49.44\n",
      "history 39.01\n",
      "history 38.91\n",
      "history 36.46\n",
      "mens 0\n",
      "zip_code_surburban 0\n",
      "recency 2\n",
      "recency 4\n",
      "history 50.13\n",
      "history 64.69\n",
      "history 64.21\n",
      "channel_phone 0\n",
      "history 78.55\n",
      "history_segment_2)_$100__$200 0\n",
      "mens 0\n",
      "history 81.75\n",
      "recency 3\n",
      "zip_code_surburban 0\n",
      "zip_code_surburban 0\n",
      "womens 0\n",
      "recency 2\n",
      "zip_code_surburban 0\n",
      "channel_phone 0\n",
      "history 588.57\n",
      "history_segment_4)_$350__$500 0\n",
      "history 84.97\n",
      "history 268.38\n",
      "history 399.42\n",
      "mens 0\n",
      "history 180.84\n",
      "history 143.66\n",
      "history 119.52\n",
      "zip_code_surburban 0\n",
      "history 137.22\n",
      "history 119.55\n",
      "history 185.36\n",
      "history 35.21\n",
      "history 68.39\n",
      "history 163.63\n",
      "history 155.17\n",
      "history 109.77\n",
      "recency 4\n",
      "recency 3\n",
      "womens 0\n",
      "mens 0\n",
      "channel_multichannel 0\n",
      "history 1564.58\n",
      "history 948.21\n",
      "history 937.62\n",
      "history 671.6\n",
      "history 616.7\n",
      "history 1134.84\n",
      "history 683.49\n",
      "history 606.9\n",
      "history 1322.67\n",
      "history 618.35\n",
      "channel_web 0\n",
      "history 1186.7\n",
      "zip_code_surburban 0\n",
      "recency 4\n",
      "zip_code_rural 0\n",
      "history 1090.48\n",
      "channel_multichannel 0\n",
      "recency 1\n",
      "history 1140.54\n",
      "channel_multichannel 0\n",
      "history 716.28\n",
      "history 751.63\n",
      "history 1074.39\n",
      "history 234.54\n",
      "channel_web 0\n",
      "zip_code_rural 0\n",
      "recency 8\n",
      "history 76.34\n",
      "history 75.21\n",
      "recency 7\n",
      "history 52.52\n",
      "recency 10\n",
      "history_segment_2)_$100__$200 0\n",
      "history 103.93\n",
      "history 97.28\n",
      "recency 8\n",
      "history 185.15\n",
      "history 155.88\n",
      "history 121.34\n",
      "history 121.15\n",
      "zip_code_urban 0\n",
      "history 106.84\n",
      "history 91.26\n",
      "history 71.89\n",
      "history 69.52\n",
      "history 49.77\n",
      "history 48.08\n",
      "recency 9\n",
      "recency 7\n",
      "mens 0\n",
      "recency 9\n",
      "mens 0\n",
      "history 29.99\n",
      "history 79.11\n",
      "history 115.07\n",
      "history 49.26\n",
      "zip_code_urban 0\n",
      "history 227.02\n",
      "history 760.17\n",
      "history 677.56\n",
      "zip_code_rural 0\n",
      "history 236.21\n",
      "history 527.87\n",
      "channel_web 0\n",
      "womens 0\n",
      "history 413.79\n",
      "history 409.39\n",
      "recency 7\n",
      "history 246.74\n",
      "zip_code_surburban 0\n",
      "history 389.77\n",
      "history 530.76\n",
      "channel_phone 0\n",
      "history 608.36\n",
      "history 606.05\n",
      "channel_web 0\n",
      "history_segment_4)_$350__$500 0\n",
      "history 769.03\n",
      "history 1380.72\n",
      "recency 7\n",
      "      pr_y1_t1  pr_y1_t0\n",
      "0     0.043478  0.040000\n",
      "1     0.053763  0.081081\n",
      "2     0.116279  0.050000\n",
      "3     0.180556  0.015625\n",
      "4     0.084507  0.078947\n",
      "5     0.166667  0.138462\n",
      "6     0.051282  0.157895\n",
      "7     0.086957  0.029851\n",
      "8     0.281690  0.037500\n",
      "9     0.006211  0.005882\n",
      "10    0.111111  0.050000\n",
      "11    0.130952  0.342857\n",
      "12    0.033333  0.197802\n",
      "13    0.078947  0.020202\n",
      "14    0.253012  0.173913\n",
      "15    0.065217  0.031250\n",
      "16    0.460526  0.139535\n",
      "17    0.037736  0.147059\n",
      "18    0.536585  0.292683\n",
      "19    0.157303  0.015625\n",
      "20    0.044776  0.010989\n",
      "21    0.050000  0.020408\n",
      "22    0.012346  0.016393\n",
      "23    0.054545  0.008547\n",
      "24    0.160494  0.070588\n",
      "25    0.041237  0.101449\n",
      "26    0.134328  0.027027\n",
      "27    0.051282  0.157895\n",
      "28    0.093750  0.025641\n",
      "29    0.212121  0.011111\n",
      "...        ...       ...\n",
      "8506  0.014286  0.009174\n",
      "8507  0.155844  0.139535\n",
      "8508  0.066667  0.041237\n",
      "8509  0.112903  0.123596\n",
      "8510  0.036364  0.170732\n",
      "8511  0.063291  0.008547\n",
      "8512  0.300000  0.088889\n",
      "8513  0.096774  0.174603\n",
      "8514  0.259259  0.300000\n",
      "8515  0.120879  0.027778\n",
      "8516  0.223404  0.057971\n",
      "8517  0.750000  0.292683\n",
      "8518  0.150000  0.151163\n",
      "8519  0.121951  0.189873\n",
      "8520  0.243902  0.200000\n",
      "8521  0.088608  0.093750\n",
      "8522  0.285714  0.200000\n",
      "8523  0.131579  0.164706\n",
      "8524  0.049383  0.022727\n",
      "8525  0.006211  0.001818\n",
      "8526  0.148148  0.133333\n",
      "8527  0.236364  0.170732\n",
      "8528  0.058824  0.022727\n",
      "8529  0.089552  0.189873\n",
      "8530  0.025641  0.056338\n",
      "8531  0.163043  0.025000\n",
      "8532  0.195402  0.031250\n",
      "8533  0.303797  0.057143\n",
      "8534  0.279070  0.040816\n",
      "8535  0.117647  0.107527\n",
      "\n",
      "[8536 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "res = uplift_tree_tma(X_train, Y_train, T_train)\n",
    "pred = predict_tree_tma(res, X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmJ20-UABApL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "code_day3_start.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
