{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehi1150I7fxN"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hH-OAGszyef5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_hilstrom = 'https://drive.google.com/open?id=15osyN4c5z1pSo1JkxwL_N8bZTksRvQuU'\n",
    "fluff, id = link_hilstrom.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('Hillstrom.csv')\n",
    "hillstrom_df = pd.read_csv('Hillstrom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tAS92JMPe9U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_lalonde = 'https://drive.google.com/open?id=1b8N7WtwIe2WmQJD1KL5UAy70K13MxwKj'\n",
    "fluff, id = link_lalonde.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('Lalonde.csv')\n",
    "lalonde_df = pd.read_csv('Lalonde.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOz5t0oZpHuU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_criteo = 'https://drive.google.com/open?id=1Vxv7JiEyFr2A99xT6vYzB5ps5WhvV7NE'\n",
    "fluff, id = link_criteo.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('criteo_small.csv')\n",
    "lalonde_df = pd.read_csv('criteo_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoNrZI5P80wJ"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "def preprocess_data(df, dataset='hillstrom', verbose=True):\n",
    "    # For Hillstrom dataset, the ‘‘visit’’ target variable was selected\n",
    "    #   as the target variable of interest and the selected treatment is \n",
    "    #   the e-mail campaign for women’s merchandise [1]\n",
    "    # [1] Kane K, Lo VSY, Zheng J. True-lift modeling: Comparison of methods. \n",
    "    #    J Market Anal. 2014;2:218–238\n",
    "    dataset = dataset.lower()\n",
    "    if dataset in ('hillstrom', 'email'):\n",
    "        columns = df.columns\n",
    "        for col in columns:\n",
    "            if df[col].dtype != object:\n",
    "                continue\n",
    "            df = pd.concat(\n",
    "                    [df, pd.get_dummies(df[col], \n",
    "                                        prefix=col, \n",
    "                                        drop_first=False)],\n",
    "                    axis=1)\n",
    "            df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        df.columns = [col.replace('-', '').replace(' ', '_').lower()\n",
    "                      for col in df.columns]\n",
    "        df = df[df.segment_mens_email == 0]\n",
    "        df.index = range(len(df))\n",
    "        df.drop(['segment_mens_email', \n",
    "                 'segment_no_email', \n",
    "                 'conversion', \n",
    "                 'spend'], axis=1, inplace=True)\n",
    "\n",
    "        y_name = 'visit'\n",
    "        t_name = 'segment_womens_email'\n",
    "    elif dataset in ['criteo', 'ad']:\n",
    "        df = df.fillna(0)\n",
    "        y_name = 'y'\n",
    "        t_name = 'treatment'\n",
    "    elif dataset == 'lalonde':\n",
    "        y_name = 'RE78'\n",
    "        t_name = 'treatment'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    df['Y'] = df[y_name]\n",
    "    df.drop([y_name], axis=1, inplace=True)\n",
    "    df['T'] = df[t_name]\n",
    "    df.drop([t_name], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Igf3QLgdJ1cW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def performance(pr_y1_t1, pr_y1_t0, y, t, groups=10):\n",
    "    \"\"\"\n",
    "    1. Split the total customers into the given number of groups\n",
    "    2. Calculate the statistics of each segment\n",
    "    \n",
    "    Args:\n",
    "        pr_y1_t1: the series (list) of the customer's expected return\n",
    "        pr_y1_t0: the expected return when a customer is not treated\n",
    "        y: the observed return of customers\n",
    "        t: whther each customer is treated or not\n",
    "        groups: the number of groups (segments). Should be 5, 10, or 20\n",
    "    Return:\n",
    "        DataFrame:\n",
    "            columns:\n",
    "                'n_y1_t1': the number of treated responders\n",
    "                'n_y1_t0': the number of not treated responders\n",
    "                'r_y1_t1': the average return of treated customers\n",
    "                'r_y1_t0': the average return of not treated customers\n",
    "                'n_t1': the number of treated customers\n",
    "                'n_t0': the number of not treated customers\n",
    "                'uplift': the average uplift (the average treatment effect)\n",
    "            rows: the index of groups\n",
    "    \"\"\"\n",
    "  \n",
    "    ### check valid arguments\n",
    "    if groups not in [5, 10, 20]:\n",
    "        raise Exception(\"uplift: groups must be either 5, 10 or 20\")\n",
    "  \n",
    "    ### check for NAs.\n",
    "    if pr_y1_t1.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in pr_y1_t1\")\n",
    "    if pr_y1_t0.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in pr_y1_t0\")\n",
    "    if y.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in y\")\n",
    "    if t.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in t\")\n",
    "   \n",
    "    ### check valid values for y and t\n",
    "    # if set(y) != {0, 1}:\n",
    "    #     raise Exception(\"uplift: y must be either 0 or 1\")\n",
    "    if set(t) != {0, 1}:\n",
    "        raise Exception(\"uplift: t must be either 0 or 1\")\n",
    "\n",
    "    ### check length of arguments\n",
    "    if not (len(pr_y1_t1) == len(pr_y1_t0) == len(y) == len(t)):\n",
    "        raise Exception(\"uplift: arguments pr_y1_t1, pr_y1_t0, y and t must all have the same length\")\n",
    "\n",
    "    ### define dif_pred\n",
    "    dif_pred = pr_y1_t1 - pr_y1_t0\n",
    "  \n",
    "    ### Make index same\n",
    "    y.index = dif_pred.index\n",
    "    t.index = dif_pred.index\n",
    "    \n",
    "    mm = pd.DataFrame({\n",
    "        'dif_pred': dif_pred,\n",
    "        'y': y,\n",
    "        't': t,\n",
    "        'dif_pred_r': dif_pred.rank(ascending=False, method='first')\n",
    "    })\n",
    "\n",
    "    mm_groupby = mm.groupby(pd.qcut(mm['dif_pred_r'], groups, labels=range(1, groups+1), duplicates='drop'))\n",
    "  \n",
    "    n_y1_t1 = mm_groupby.apply(lambda r: r[r['t'] == 1]['y'].sum())\n",
    "    n_y1_t0 = mm_groupby.apply(lambda r: r[r['t'] == 0]['y'].sum())\n",
    "    n_t1 = mm_groupby['t'].sum()\n",
    "    n_t0 = mm_groupby['t'].count() - n_t1\n",
    "  \n",
    "    df = pd.DataFrame({\n",
    "        'n_t1': n_t1,\n",
    "        'n_t0': n_t0,\n",
    "        'n_y1_t1': n_y1_t1,\n",
    "        'n_y1_t0': n_y1_t0,\n",
    "        'r_y1_t1': n_y1_t1 / n_t1,\n",
    "        'r_y1_t0': n_y1_t0 / n_t0,\n",
    "    })\n",
    "    fillna_columns = ['n_y1_t1', 'n_y1_t0', 'n_t1', 'n_t0']\n",
    "    df[fillna_columns] = df[fillna_columns].fillna(0)\n",
    "    df.index.name = 'groups'\n",
    "\n",
    "    df['uplift'] = df['r_y1_t1'] - df['r_y1_t0']\n",
    "    df['uplift'] = round(df['uplift'], 6)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def qini(perf, plotit=True):\n",
    "    nrow = len(perf)\n",
    "\n",
    "    # Calculating the incremental gains. \n",
    "    # - First, the cumulitative sum of the treated and the control groups are\n",
    "    #  calculated with respect to the total population in each group at the\n",
    "    #  specified decile\n",
    "    # - Afterwards we calculate the percentage of the total amount of people\n",
    "    #  (both treatment and control) are present in each decile\n",
    "    cumul_y1_t1 = (perf['n_y1_t1'].cumsum() / perf['n_t1'].cumsum()).fillna(0)\n",
    "    cumul_y1_t0 = (perf['n_y1_t0'].cumsum() / perf['n_t0'].cumsum()).fillna(0)\n",
    "    deciles = [i/nrow for i in range(1, nrow+1)]\n",
    "\n",
    "    ### Model Incremental gains\n",
    "    inc_gains = (cumul_y1_t1 - cumul_y1_t0) * deciles\n",
    "    inc_gains = [0.0] + list(inc_gains)\n",
    "\n",
    "    ### Overall incremental gains\n",
    "    overall_inc_gain = sum(perf['n_y1_t1']) / sum(perf['n_t1']) \\\n",
    "            - sum(perf['n_y1_t0']) / sum(perf['n_t0'])\n",
    "\n",
    "    ### Random incremental gains\n",
    "    random_inc_gains = [i*overall_inc_gain / nrow for i in range(nrow+1)]\n",
    "\n",
    "    ### Compute area under the model incremental gains (uplift) curve\n",
    "    x = [0] + deciles\n",
    "    y = list(inc_gains)\n",
    "    auuc = 0\n",
    "    auuc_rand = 0\n",
    "\n",
    "    auuc_list = [auuc]\n",
    "    for i in range(1, len(x)):\n",
    "        auuc += 0.5 * (x[i] - x[i-1]) * (y[i] + y[i-1])\n",
    "        auuc_list.append(auuc)\n",
    "\n",
    "    ### Compute area under the random incremental gains curve\n",
    "    y_rand = random_inc_gains\n",
    "\n",
    "    auuc_rand_list = [auuc_rand]\n",
    "    for i in range(1, len(x)):\n",
    "        auuc_rand += 0.5 * (x[i] - x[i-1]) * (y_rand[i] + y_rand[i-1])\n",
    "        auuc_rand_list.append(auuc_rand)\n",
    "\n",
    "    ### Compute the difference between the areas (Qini coefficient)\n",
    "    Qini = auuc - auuc_rand\n",
    "\n",
    "    ### Plot incremental gains curve\n",
    "    if plotit:\n",
    "        x_axis = x\n",
    "        plt.plot(x_axis, inc_gains)\n",
    "        plt.plot(x_axis, random_inc_gains)\n",
    "        plt.show()\n",
    "    \n",
    "    ### Qini 30%, Qini 10%\n",
    "    n_30p = int(nrow*3/10)\n",
    "    n_10p = int(nrow/10)\n",
    "    qini_30p = auuc_list[n_30p] - auuc_rand_list[n_30p]\n",
    "    qini_10p = auuc_list[n_10p] - auuc_rand_list[n_10p]\n",
    "\n",
    "    res = {\n",
    "        'qini': Qini,\n",
    "        'inc_gains': inc_gains,\n",
    "        'random_inc_gains': random_inc_gains,\n",
    "        'auuc_list': auuc_list,\n",
    "        'auuc_rand_list': auuc_rand_list,\n",
    "        'qini_30p': qini_30p,\n",
    "        'qini_10p': qini_10p,\n",
    "    }    \n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nrxjl1v7J9Mm"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def parameter_tuning(fit_mdl, pred_mdl, data, search_space, plotit=False):\n",
    "    \"\"\"\n",
    "    Given a model, search all combination of parameter sets and find\n",
    "    the best parameter set\n",
    "    \n",
    "    Args:\n",
    "        fit_mdl: model function\n",
    "        pred_mdl: predict function of fit_mdl\n",
    "        data:\n",
    "            {\n",
    "                \"x_train\": predictor variables of training dataset,\n",
    "                \"y_train\": target variables of training dataset,\n",
    "                \"t_train\": treatment variables of training dataset,\n",
    "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
    "                \"y_test\": target variables of test (usually, validation) dataset,\n",
    "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
    "            }\n",
    "        search_space:\n",
    "            {\n",
    "                parameter_name: [search values]\n",
    "            }\n",
    "    Return:\n",
    "        The best parameter set\n",
    "    \"\"\"\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    t_train = data['t_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "    t_test = data['t_test']\n",
    "    \n",
    "    max_q = -float('inf')\n",
    "    best_mdl = None\n",
    "\n",
    "    keys = search_space.keys()\n",
    "    n_space = [len(search_space[key]) for key in keys]\n",
    "    n_iter = np.prod(n_space)\n",
    "    \n",
    "    best_params = None\n",
    "    for i in range(n_iter):\n",
    "        params = {}\n",
    "        for idx, key in enumerate(keys):\n",
    "            params[key] = search_space[key][i % n_space[idx]]\n",
    "            i = int(i / n_space[idx])\n",
    "\n",
    "        mdl = fit_mdl(x_train, y_train, t_train, **params)\n",
    "        pred = pred_mdl(mdl, newdata=x_test, y=y_test, t=t_test)\n",
    "        # print('    {}'.format(params))\n",
    "        try:\n",
    "            perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        q = qini(perf, plotit=plotit)['qini']\n",
    "        if plotit:\n",
    "            print(q, params)\n",
    "        if q > max_q:\n",
    "            max_q = q\n",
    "            best_mdl = mdl\n",
    "            best_params = params\n",
    "\n",
    "    return best_mdl, best_params\n",
    "\n",
    "\n",
    "def wrapper(fit_mdl, pred_mdl, data, params=None,\n",
    "            best_models=None, drop_variables=None, qini_values=None):\n",
    "    \"\"\"\n",
    "    General wrapper approach\n",
    "    \n",
    "    Args:\n",
    "        fit_mdl: model function\n",
    "        pred_mdl: predict function of fit_mdl\n",
    "        data:\n",
    "            {\n",
    "                \"x_train\": predictor variables of training dataset,\n",
    "                \"y_train\": target variables of training dataset,\n",
    "                \"t_train\": treatment variables of training dataset,\n",
    "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
    "                \"y_test\": target variables of test (usually, validation) dataset,\n",
    "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
    "            }\n",
    "    Return:\n",
    "        (A list of best models, The list of dropped variables)\n",
    "    \"\"\"\n",
    "    if best_models is None:\n",
    "        best_models = []\n",
    "    if drop_variables is None:\n",
    "        drop_variables = []\n",
    "    if qini_values is None:\n",
    "        qini_values = []\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    t_train = data['t_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "    t_test = data['t_test']\n",
    "\n",
    "    variables = data['x_train'].columns\n",
    "\n",
    "    max_q = -float('inf')\n",
    "    drop_var = None\n",
    "    best_mdl = None\n",
    "    for var in variables:\n",
    "        if var in drop_variables:\n",
    "            continue\n",
    "        x = x_train.copy()\n",
    "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
    "        mdl = fit_mdl(x, y_train, t_train, **params)\n",
    "        x = x_test.copy()\n",
    "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
    "        pred = pred_mdl(mdl, newdata=x, y=y_test, t=t_test)\n",
    "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
    "        q = qini(perf, plotit=False)['qini']\n",
    "        if q > max_q:\n",
    "            max_q = q\n",
    "            drop_var = var\n",
    "            best_mdl = mdl\n",
    "    \n",
    "    best_models.append(best_mdl)\n",
    "    drop_variables.append(drop_var)\n",
    "    qini_values.append(max_q)\n",
    "\n",
    "    left_vars = [var for var in variables if (var not in drop_variables)]\n",
    "    \n",
    "    if len(variables) == len(drop_variables) + 1:\n",
    "        return best_models, drop_variables + left_vars, qini_values\n",
    "    else:\n",
    "        return wrapper(fit_mdl, pred_mdl, data, params=params,\n",
    "                       best_models=best_models, drop_variables=drop_variables,\n",
    "                       qini_values=qini_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxrEbEODlbQi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def tma(x, y, t, method=LogisticRegression, **kwargs):\n",
    "    \"\"\"Training a model according to the \"Two Model Approach\" \n",
    "    (a.k.a. \"Separate Model Approach\")\n",
    "    The default model is General Linear Model (GLM)\n",
    "    \n",
    "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
    "\n",
    "    Args:\n",
    "        x: A data frame of predictors.\n",
    "        y: A binary response (numeric) vector.\n",
    "        t: A binary response (numeric) representing the treatment assignment\n",
    "            (coded as 0/1).\n",
    "        method: A sklearn model specifying which classification or regression\n",
    "            model to use. This should be a method that can handle a \n",
    "            multinominal class variable.\n",
    "\n",
    "    Return:\n",
    "        Dictionary: A dictionary of two models. One for the treatment group, \n",
    "            one for the control group.\n",
    "\n",
    "            {\n",
    "                'model_treat': a model for the treatment group,\n",
    "                'model_control': a model for the control group\n",
    "            }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    treat_rows = (t == 1)\n",
    "    control_rows = (t == 0)\n",
    "    model_treat = method(**kwargs).fit(x[treat_rows], y[treat_rows])\n",
    "    model_control = method(**kwargs).fit(x[control_rows], y[control_rows])\n",
    "    \n",
    "    res = {\n",
    "        'model_treat': model_treat,\n",
    "        'model_control': model_control,\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "def predict_tma(obj, newdata, **kwargs):\n",
    "    \"\"\"Predictions according to the \"Two Model Approach\" \n",
    "    (a.k.a. \"Separate Model Approach\")\n",
    "    \n",
    "    For each instance in newdata two predictions are made:\n",
    "    1) What is the probability of a person responding when treated?\n",
    "    2) What is the probability of a person responding when not treated\n",
    "      (i.e. part of control group)?\n",
    "\n",
    "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
    "\n",
    "    Args:\n",
    "        obj: A dictionary of two models. \n",
    "            One for the treatment group, one for the control group.\n",
    "        newdata: A data frame containing the values at which predictions\n",
    "            are required.\n",
    "    \n",
    "    Return:\n",
    "        DataFrame: A dataframe with predicted returns for when the customers\n",
    "            are treated and for when they are not treated.\n",
    "    \"\"\"\n",
    "   \n",
    "    if isinstance(obj['model_treat'], LinearRegression):\n",
    "        pred_treat = obj['model_treat'].predict(newdata)\n",
    "    else:\n",
    "        pred_treat = obj['model_treat'].predict_proba(newdata)[:, 1]\n",
    "\n",
    "    if isinstance(obj['model_control'], LinearRegression):\n",
    "        pred_control = obj['model_control'].predict(newdata)\n",
    "    else:\n",
    "        pred_control = obj['model_control'].predict_proba(newdata)[:, 1]\n",
    "    \n",
    "    # pred_treat = obj['model_treat'].predict(newdata)\n",
    "    # pred_control = obj['model_control'].predict(newdata)\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"pr_y1_t1\": pred_treat,\n",
    "        \"pr_y1_t0\": pred_control,\n",
    "    })\n",
    "    return pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hREl_CRv9DYC"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def ty_assign(y, t):\n",
    "    if y == 1 and t == 1:\n",
    "        return \"TR\"\n",
    "    elif y == 0 and t == 1:\n",
    "        return \"TN\"\n",
    "    elif y == 1 and t == 0:\n",
    "        return \"CR\"\n",
    "    elif y == 0 and t == 0:\n",
    "        return \"CN\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def t_assign(ty):\n",
    "    if ty in (\"TR\", \"TN\"):\n",
    "        return 1\n",
    "    elif ty in (\"CR\", \"CN\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def y_assign(ty):\n",
    "    if ty in (\"TR\", \"CR\"):\n",
    "        return 1\n",
    "    elif ty in (\"TN\", \"CN\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9X42GUaj-UX"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, attribute, threshold):\n",
    "        self.attr = attribute\n",
    "        self.thres = threshold\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf = False\n",
    "        self.predict = None, None\n",
    "\n",
    "\n",
    "def info_gain(df, attribute, predict_attr, treatment_attr,\n",
    "              method, min_bucket_t0, min_bucket_t1):\n",
    "    \"\"\"\n",
    "    Select the information gain and threshold of the attribute to split\n",
    "    The threshold chosen splits the test data such that information gain is maximized\n",
    "    \n",
    "    Return a pandas.DataFrame\n",
    "        columns: 'thres' (threshold) and 'info_gain' (information gain)\n",
    "    \"\"\"\n",
    "    num_total = df.shape[0]\n",
    "    tmp = pd.DataFrame({\n",
    "        'thres': df[attribute],\n",
    "        'Y': df[predict_attr],\n",
    "        'T': df[treatment_attr]\n",
    "    })\n",
    "    tmp.sort_values(['thres'], inplace=True)\n",
    "\n",
    "    tmp['n_t1_L'] = (tmp['T']).cumsum()\n",
    "    tmp['n_t0_L'] = (tmp['T'] == 0).cumsum()\n",
    "    tmp['n_t1_R'] = sum(tmp['T']) - (tmp['T']).cumsum()\n",
    "    tmp['n_t0_R'] = sum(tmp['T'] == 0) - (tmp['T'] == 0).cumsum()\n",
    "    tmp['n_y_t1_L'] = (tmp['T'] & tmp['Y']).cumsum()\n",
    "    tmp['n_y_t0_L'] = ((tmp['T'] == 0) & tmp['Y']).cumsum()\n",
    "    tmp['n_y_t1_R'] = sum(tmp['T'] & tmp['Y']) - (tmp['T'] & tmp['Y']).cumsum()\n",
    "    tmp['n_y_t0_R'] = sum((tmp['T'] == 0) & tmp['Y']) - ((tmp['T'] == 0) & tmp['Y']).cumsum()\n",
    "        \n",
    "    # min bucket condition\n",
    "    #   Check the size of treatment & control group in left & right child\n",
    "    tmp['min_bucket_ok'] = ((tmp['n_t1_L'] >= min_bucket_t1) & \n",
    "                            (tmp['n_t0_L'] >= min_bucket_t0) &\n",
    "                            (tmp['n_t1_R'] >= min_bucket_t1) &\n",
    "                            (tmp['n_t0_R'] >= min_bucket_t0))\n",
    "    \n",
    "    if sum(tmp['min_bucket_ok']) > 0:\n",
    "        num_total = df.shape[0]\n",
    "        tr, tn, cr, cn = num_class(df, predict_attr, treatment_attr)\n",
    "        n_t1 = tr + tn\n",
    "        n_t0 = cr + cn\n",
    "        pr_t1 = (tr + tn) / (num_total)\n",
    "        # r_t0 = (tr + cr) / (num_total)\n",
    "        pr_t0 = 1 - pr_t1\n",
    "        pr_y1_t1 = tr / (tr + tn)\n",
    "        pr_y1_t0 = cr / (cr + cn)\n",
    "\n",
    "        # Randomized assignment implies pr_l_t1 = pr_l_t0 for all possible splits\n",
    "        pr_l_t1 = (tmp['n_t1_L']) / (n_t1)\n",
    "        pr_l_t0 = (tmp['n_t0_L']) / (n_t0)\n",
    "        pr_l = pr_l_t1 * pr_t1 + pr_l_t0 * pr_t0\n",
    "        pr_r = 1 - pr_l\n",
    "\n",
    "        # Add Laplace correction to probablities\n",
    "        pr_y1_l_t1 = (tmp['n_y_t1_L']) / (tmp['n_t1_L'])\n",
    "        pr_y1_l_t0 = (tmp['n_y_t0_L']) / (tmp['n_t0_L'])\n",
    "        pr_y1_r_t1 = (tmp['n_y_t1_R']) / (tmp['n_t1_R'])\n",
    "        pr_y1_r_t0 = (tmp['n_y_t0_R']) / (tmp['n_t0_R'])\n",
    "\n",
    "        # Number of treatment/control observations at left and right child nodes\n",
    "        n_t1_L = tmp['n_t1_L']\n",
    "        n_t0_L = tmp['n_t0_L']\n",
    "        n_t1_R = tmp['n_t1_R']\n",
    "        n_t0_R = tmp['n_t0_R']\n",
    "\n",
    "        if method.lower() == 'ed':\n",
    "            tmp['info_gain'] = eucli_dist(tmp,\n",
    "                                          pr_y1_t1,\n",
    "                                          pr_y1_t0,\n",
    "                                          pr_l,\n",
    "                                          pr_r,\n",
    "                                          pr_y1_l_t1,\n",
    "                                          pr_y1_l_t0,\n",
    "                                          pr_y1_r_t1,\n",
    "                                          pr_y1_r_t0,\n",
    "                                          pr_t1,\n",
    "                                          pr_t0,\n",
    "                                          pr_l_t1,\n",
    "                                          pr_l_t0)\n",
    "        elif method.lower() == 'kl':\n",
    "            tmp['info_gain'] = kl_divergence(tmp,\n",
    "                                            pr_y1_t1,\n",
    "                                            pr_y1_t0,\n",
    "                                            pr_l,\n",
    "                                            pr_r,\n",
    "                                            pr_y1_l_t1,\n",
    "                                            pr_y1_l_t0,\n",
    "                                            pr_y1_r_t1,\n",
    "                                            pr_y1_r_t0,\n",
    "                                            pr_t1,\n",
    "                                            pr_t0,\n",
    "                                            pr_l_t1,\n",
    "                                            pr_l_t0)\n",
    "        elif method.lower() == 'chisq':\n",
    "            tmp['info_gain'] = chisq(tmp,\n",
    "                                    pr_y1_t1,\n",
    "                                    pr_y1_t0,\n",
    "                                    pr_l,\n",
    "                                    pr_r,\n",
    "                                    pr_y1_l_t1,\n",
    "                                    pr_y1_l_t0,\n",
    "                                    pr_y1_r_t1,\n",
    "                                    pr_y1_r_t0,\n",
    "                                    pr_t1,\n",
    "                                    pr_t0,\n",
    "                                    pr_l_t1,\n",
    "                                    pr_l_t0)\n",
    "        elif method.lower() == 'int':\n",
    "            tmp['info_gain'] = interaction_split(tmp,\n",
    "                                                pr_y1_t1,\n",
    "                                                pr_y1_t0,\n",
    "                                                pr_l,\n",
    "                                                pr_r,\n",
    "                                                pr_y1_l_t1,\n",
    "                                                pr_y1_l_t0,\n",
    "                                                pr_y1_r_t1,\n",
    "                                                pr_y1_r_t0,\n",
    "                                                pr_t1,\n",
    "                                                pr_t0,\n",
    "                                                pr_l_t1,\n",
    "                                                pr_l_t0,\n",
    "                                                n_t1_L,\n",
    "                                                n_t0_L,\n",
    "                                                n_t1_R,\n",
    "                                                n_t0_R)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    # We will select one rows per one distinct candidate\n",
    "    tmp['dups'] = tmp['thres'].duplicated(keep='last')\n",
    "    tmp['thres_ok'] = (tmp['min_bucket_ok'] & (tmp['dups'] == False))\n",
    "    tmp.dropna(inplace=True)\n",
    "    if sum(tmp['thres_ok']) < 1:\n",
    "        return None\n",
    "\n",
    "    tmp = tmp[tmp['thres_ok']]\n",
    "\n",
    "    return tmp[['thres', 'info_gain']]\n",
    "    \n",
    "\n",
    "\n",
    "def num_class(df, predict_attr, treatment_attr):\n",
    "    \"\"\"\n",
    "    Returns the number of Responders and Non-responders in Treatment and Control group\n",
    "    \"\"\"\n",
    "    tr = df[(df[predict_attr] == 1) & (df[treatment_attr] == 1)]  # Responders in Treatment group\n",
    "    tn = df[(df[predict_attr] == 0) & (df[treatment_attr] == 1)]  # Non-responders in Treatment group\n",
    "    cr = df[(df[predict_attr] == 1) & (df[treatment_attr] == 0)]  # Responders in Control group\n",
    "    cn = df[(df[predict_attr] == 0) & (df[treatment_attr] == 0)]  # Non-responders in Control group\n",
    "    return tr.shape[0], tn.shape[0], cr.shape[0], cn.shape[0]\n",
    "\n",
    "\n",
    "def choose_attr(df, attributes, predict_attr, treatment_attr,\n",
    "                method, min_bucket_t0, min_bucket_t1):\n",
    "    \"\"\"\n",
    "    Chooses the attribute and its threshold with the highest info gain\n",
    "    from the set of attributes\n",
    "    \"\"\"\n",
    "    max_info_gain = 0\n",
    "    best_attr = None\n",
    "    threshold = None\n",
    "    # Test each attribute (note attributes maybe be chosen more than once)\n",
    "    for attr in attributes:\n",
    "        df_ig = info_gain(df, attr, predict_attr, treatment_attr,\n",
    "                          method, min_bucket_t0, min_bucket_t1)\n",
    "        if df_ig is None:\n",
    "            continue\n",
    "\n",
    "        # Get the possible indices of maximum info gain\n",
    "        ig = max(df_ig['info_gain'])\n",
    "        idx_ig = df_ig.index[df_ig['info_gain']==ig]\n",
    "        # Break ties randomly\n",
    "        idx_ig = random.choice(idx_ig)\n",
    "        # Get information gain & threshold of that\n",
    "        thres = df_ig['thres'][idx_ig]\n",
    "\n",
    "        if ig > max_info_gain:\n",
    "            max_info_gain = ig\n",
    "            best_attr = attr\n",
    "            threshold = thres\n",
    "    return best_attr, threshold\n",
    "\n",
    "\n",
    "def build_tree(df, cols, predict_attr='Y', treatment_attr='T',\n",
    "               method='ED', depth=1, max_depth=float('INF'),\n",
    "               min_split=2000, min_bucket_t0=None, min_bucket_t1=None,\n",
    "               mtry=None, random_seed=1234):\n",
    "    \"\"\"\n",
    "    Builds the Decision Tree based on training data, attributes to train on,\n",
    "    and a prediction attribute\n",
    "    \"\"\"\n",
    "    if depth == 1:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    if mtry is None:\n",
    "        mtry = math.floor(math.sqrt(len(cols)))\n",
    "    if min_bucket_t0 is None:\n",
    "        min_bucket_t0 = round(min_split/4)\n",
    "    if min_bucket_t1 is None:\n",
    "        min_bucket_t1 = round(min_split/4)\n",
    "    \n",
    "    # Get the number of positive and negative examples in the training data\n",
    "    tr, tn, cr, cn = num_class(df, predict_attr, treatment_attr)\n",
    "    r_y1_ct1 = tr / (tr + tn)\n",
    "    r_y1_ct0 = cr / (cr + cn)\n",
    "\n",
    "    # Check varialbes have less than 2 levels at the current node\n",
    "    # If not, exclude them as candidates for mtry selection\n",
    "    # To split the node, sum(ok_vars) should be equal or larger than self.mtry\n",
    "    ok_vars = []\n",
    "    for col in cols:\n",
    "        ok_vars.append(len(set(df[col])) > 1)\n",
    "\n",
    "    # Whether we have to split this node\n",
    "    #   1. min split condition: Both the sizes of treatment and control group \n",
    "    #     of an internal node should be larger than 'min_split'\n",
    "    #   2. max depth condition: The depth of tree is 'max_depth'\n",
    "    #   3. min_bucket condition: The number of treatment/control group of a\n",
    "    #     node should be larger than 'min_bucket_t0'/'min_bucket_t1'\n",
    "    #   4. Expected return should be larger than 0 and smaller than 1\n",
    "    #     (for KL-divergence & Chisq splitting criteria)\n",
    "    split_cond = tr + tn > min_split and cr + cn > min_split \\\n",
    "            and 0 < r_y1_ct1 < 1 and 0 < r_y1_ct0 < 1 \\\n",
    "            and depth < max_depth and sum(ok_vars) >= mtry\n",
    "    \n",
    "    best_attr, threshold = None, None\n",
    "    if split_cond:\n",
    "        # Sample columns\n",
    "        ok_cols = [col for col in cols if len(set(df[col])) > 1]\n",
    "        ok_cols = np.random.choice(ok_cols, mtry, replace=False)\n",
    "        # Determine attribute and its threshold value with the highest\n",
    "        # information gain\n",
    "        best_attr, threshold = choose_attr(df, ok_cols, predict_attr, treatment_attr,\n",
    "                                           method, min_bucket_t0, min_bucket_t1)\n",
    "    if best_attr is None:\n",
    "        # Create a leaf node indicating it's prediction\n",
    "        leaf = Node(None,None)\n",
    "        leaf.leaf = True\n",
    "        leaf.predict = (tr / (tr + tn), cr / (cr + cn))\n",
    "        return leaf\n",
    "    else:\n",
    "        # Create internal tree node based on attribute and it's threshold\n",
    "        sub_1 = df[df[best_attr] <= threshold]\n",
    "        sub_2 = df[df[best_attr] > threshold]\n",
    "        sub1_tr, sub1_tn, sub1_cr, sub1_cn = num_class(sub_1, predict_attr, treatment_attr)\n",
    "        sub2_tr, sub2_tn, sub2_cr, sub2_cn = num_class(sub_2, predict_attr, treatment_attr)\n",
    "        tree = Node(best_attr, threshold)\n",
    "        # Recursively build left and right subtree\n",
    "        tree.left = build_tree(sub_1, cols, predict_attr, treatment_attr,\n",
    "                               method=method, depth=depth+1, max_depth=max_depth,\n",
    "                               min_split=min_split, min_bucket_t0=min_bucket_t0, \n",
    "                               min_bucket_t1=min_bucket_t1, mtry=mtry)\n",
    "        tree.right = build_tree(sub_2, cols, predict_attr, treatment_attr,\n",
    "                               method=method, depth=depth+1, max_depth=max_depth,\n",
    "                               min_split=min_split, min_bucket_t0=min_bucket_t0, \n",
    "                               min_bucket_t1=min_bucket_t1, mtry=mtry)\n",
    "        return tree\n",
    "\n",
    "\n",
    "def predict(node, row_df):\n",
    "    \"\"\"\n",
    "    Given a instance of a training data, make a prediction of an observation (row)\n",
    "    based on the Decision Tree\n",
    "    Assumes all data has been cleaned (i.e. no NULL data)\n",
    "    \"\"\"\n",
    "    # If we are at a leaf node, return the prediction of the leaf node\n",
    "    if node.leaf:\n",
    "        return node.predict\n",
    "    # Traverse left or right subtree based on instance's data\n",
    "    if row_df[node.attr] <= node.thres:\n",
    "        return predict(node.left, row_df)\n",
    "    elif row_df[node.attr] > node.thres:\n",
    "        return predict(node.right, row_df)\n",
    "\n",
    "\n",
    "def test_predictions(root, df):\n",
    "    \"\"\"\n",
    "    Given a set of data, make a prediction for each instance using the Decision Tree\n",
    "    \"\"\"\n",
    "    pred_treat = []\n",
    "    pred_control = []\n",
    "    for index,row in df.iterrows():\n",
    "        return_treated, return_control = predict(root, row)\n",
    "        pred_treat.append(return_treated)\n",
    "        pred_control.append(return_control)\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"pr_y1_t1\": pred_treat,\n",
    "        \"pr_y1_t0\": pred_control,\n",
    "    })\n",
    "    return pred_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZouB440A56W"
   },
   "outputs": [],
   "source": [
    "def eucli_dist(small_df,\n",
    "               pr_y1_ct1,\n",
    "               pr_y1_ct0,\n",
    "               pr_l,\n",
    "               pr_r,\n",
    "               pr_y1_l_ct1,\n",
    "               pr_y1_l_ct0,\n",
    "               pr_y1_r_ct1,\n",
    "               pr_y1_r_ct0,\n",
    "               pr_ct1,\n",
    "               pr_ct0,\n",
    "               pr_l_ct1,\n",
    "               pr_l_ct0):\n",
    "    # Euclidean gain\n",
    "    ed_node = (pr_y1_ct1 - pr_y1_ct0) ** 2 + ((1 - pr_y1_ct1) - (1 - pr_y1_ct0)) ** 2\n",
    "    ed_l = (pr_y1_l_ct1 - pr_y1_l_ct0) ** 2 + ((1 - pr_y1_l_ct1) - (1 - pr_y1_l_ct0)) ** 2\n",
    "    ed_r = (pr_y1_r_ct1 - pr_y1_r_ct0) ** 2 + ((1 - pr_y1_r_ct1) - (1 - pr_y1_r_ct0)) ** 2\n",
    "    ed_lr = pr_l * ed_l + pr_r * ed_r\n",
    "    ed_gain = ed_lr - ed_node\n",
    "\n",
    "    # Euclidean Normalization factor\n",
    "    gini_ct = 2 * pr_ct1 * (1 - pr_ct1)\n",
    "    ed_ct = (pr_l_ct1 - pr_l_ct0) ** 2 + ((1 - pr_l_ct1) - (1 - pr_l_ct0)) ** 2\n",
    "    gini_ct1 = 2 * pr_l_ct1 * (1 - pr_l_ct1)\n",
    "    gini_ct0 = 2 * pr_l_ct0 * (1 - pr_l_ct0)\n",
    "    ed_norm = gini_ct * ed_ct + gini_ct1 * pr_ct1 + gini_ct0 * pr_ct0 + 0.5\n",
    "    \n",
    "    # Output\n",
    "    info_gain_t = ed_gain / ed_norm\n",
    "\n",
    "    return info_gain_t\n",
    "\n",
    "\n",
    "def kl_divergence(small_df,\n",
    "                  pr_y1_ct1,\n",
    "                  pr_y1_ct0,\n",
    "                  pr_l,\n",
    "                  pr_r,\n",
    "                  pr_y1_l_ct1,\n",
    "                  pr_y1_l_ct0,\n",
    "                  pr_y1_r_ct1,\n",
    "                  pr_y1_r_ct0,\n",
    "                  pr_ct1,\n",
    "                  pr_ct0,\n",
    "                  pr_l_ct1,\n",
    "                  pr_l_ct0):\n",
    "    # KL Gain\n",
    "    kl_node = pr_y1_ct1 * np.log2(pr_y1_ct1/pr_y1_ct0) + \\\n",
    "             (1 - pr_y1_ct1) * np.log2((1 - pr_y1_ct1) / (1 - pr_y1_ct0))\n",
    "    kl_l = pr_y1_l_ct1 * np.log2(pr_y1_l_ct1 / pr_y1_l_ct0) + \\\n",
    "          (1 - pr_y1_l_ct1) * np.log2((1 - pr_y1_l_ct1) / (1 - pr_y1_l_ct0))\n",
    "    kl_r = pr_y1_r_ct1 * np.log2(pr_y1_r_ct1 / pr_y1_r_ct0) + \\\n",
    "          (1 - pr_y1_r_ct1) * np.log2((1 - pr_y1_r_ct1) / (1 - pr_y1_r_ct0))\n",
    "    kl_lr = pr_l * kl_l + pr_r * kl_r\n",
    "    kl_gain = kl_lr - kl_node\n",
    "\n",
    "    # KL Normalization factor\n",
    "    ent_ct = -(pr_ct1 * np.log2(pr_ct1) + pr_ct0 * np.log2(pr_ct0))\n",
    "    kl_ct = pr_l_ct1 * np.log2(pr_l_ct1 / pr_l_ct0) + \\\n",
    "           (1 - pr_l_ct1) * np.log2 ((1 - pr_l_ct1) / (1 - pr_l_ct0))\n",
    "    ent_ct1 = -(pr_l_ct1 * np.log2(pr_l_ct1) + (1 - pr_l_ct1) * np.log2((1 - pr_l_ct1)))\n",
    "    ent_ct0 = -(pr_l_ct0 * np.log2(pr_l_ct0) + (1 - pr_l_ct0) * np.log2((1 - pr_l_ct0)))\n",
    "\n",
    "    norm = kl_ct * ent_ct + ent_ct1 * pr_ct1 + ent_ct0 * pr_ct0 + 0.5\n",
    "\n",
    "    # Output\n",
    "    info_gain_t = kl_gain / norm\n",
    "\n",
    "    return info_gain_t\n",
    "\n",
    "\n",
    "def chisq(small_df,\n",
    "          pr_y1_ct1,\n",
    "          pr_y1_ct0,\n",
    "          pr_l,\n",
    "          pr_r,\n",
    "          pr_y1_l_ct1,\n",
    "          pr_y1_l_ct0,\n",
    "          pr_y1_r_ct1,\n",
    "          pr_y1_r_ct0,\n",
    "          pr_ct1,\n",
    "          pr_ct0,\n",
    "          pr_l_ct1,\n",
    "          pr_l_ct0):\n",
    "    # Chi-squared gain\n",
    "    chisq_node = ((pr_y1_ct1 - pr_y1_ct0) ** 2) / pr_y1_ct0 + \\\n",
    "                (((1 - pr_y1_ct1) - (1 - pr_y1_ct0)) ** 2) / (1 - pr_y1_ct0) \n",
    "    chisq_l = ((pr_y1_l_ct1 - pr_y1_l_ct0) ** 2) / pr_y1_l_ct0 + \\\n",
    "             (((1 - pr_y1_l_ct1) - (1 - pr_y1_l_ct0)) ** 2) / (1 - pr_y1_l_ct0)\n",
    "    chisq_r = ((pr_y1_r_ct1 - pr_y1_r_ct0) ** 2) / pr_y1_r_ct0 + \\\n",
    "             (((1 - pr_y1_r_ct1) - (1 - pr_y1_r_ct0)) ** 2) / (1 - pr_y1_r_ct0)\n",
    "    chisq_lr = pr_l * chisq_l + pr_r * chisq_r\n",
    "    chisq_gain = chisq_lr - chisq_node\n",
    "\n",
    "    # Chi-squared Normalization factor\n",
    "    gini_ct = 2 * pr_ct1 * (1 - pr_ct1) \n",
    "    chisq_ct = ((pr_l_ct1 - pr_l_ct0) ** 2) / pr_l_ct0 + \\\n",
    "              (((1 - pr_l_ct1) - (1 - pr_l_ct0)) ** 2) / (1 - pr_l_ct0)\n",
    "    gini_ct1 = 2 * pr_l_ct1 * (1 - pr_l_ct1)\n",
    "    gini_ct0 = 2 * pr_l_ct0 * (1 - pr_l_ct0)\n",
    "    chisq_norm = gini_ct * chisq_ct + gini_ct1 * pr_ct1  + gini_ct0 * pr_ct0 + 0.5\n",
    "     \n",
    "    # Output\n",
    "    info_gain_t = chisq_gain / chisq_norm\n",
    "\n",
    "    return info_gain_t\n",
    "\n",
    "\n",
    "def interaction_split(small_df,\n",
    "                      pr_y1_ct1,\n",
    "                      pr_y1_ct0,\n",
    "                      pr_l,\n",
    "                      pr_r,\n",
    "                      pr_y1_l_ct1,\n",
    "                      pr_y1_l_ct0,\n",
    "                      pr_y1_r_ct1,\n",
    "                      pr_y1_r_ct0,\n",
    "                      pr_ct1,\n",
    "                      pr_ct0,\n",
    "                      pr_l_ct1,\n",
    "                      pr_l_ct0,\n",
    "                      cs_ct1,\n",
    "                      cs_ct0,\n",
    "                      ncs_ct1,\n",
    "                      ncs_ct0):\n",
    "    # Compute elements for split formula\n",
    "    C44 = 1/cs_ct1 + 1/cs_ct0 + 1/ncs_ct1 + 1/ncs_ct0\n",
    "\n",
    "    UR = pr_y1_r_ct1 - pr_y1_r_ct0\n",
    "    UL = pr_y1_l_ct1 - pr_y1_l_ct0\n",
    "\n",
    "    SSE = cs_ct1 * pr_y1_l_ct1 * (1 - pr_y1_l_ct1) + \\\n",
    "         ncs_ct1 * pr_y1_r_ct1 * (1 - pr_y1_r_ct1) + \\\n",
    "         cs_ct0 * pr_y1_l_ct0 * (1 - pr_y1_l_ct0)  + \\\n",
    "         ncs_ct0 * pr_y1_r_ct0 * (1 - pr_y1_r_ct0)\n",
    "         \n",
    "    n_node = len(small_df)       \n",
    "\n",
    "    # Output: Interaction split\n",
    "    info_gain_t = ((n_node - 4) * (UR - UL)**2) / (C44 * SSE)\n",
    "\n",
    "    return info_gain_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ORX-_PRzfNw"
   },
   "outputs": [],
   "source": [
    "def uplift_tree(x, y, t, **kwargs):\n",
    "    predict_attr = kwargs.get('predict_attr', 'Y')\n",
    "    treatment_attr = kwargs.get('treatment_attr', 'T')\n",
    "    \n",
    "    df = x.copy()\n",
    "    df[predict_attr] = y\n",
    "    df[treatment_attr] = t\n",
    "    \n",
    "    kwargs['predict_attr'] = predict_attr\n",
    "    kwargs['treatment_attr'] = treatment_attr\n",
    "    root = build_tree(df, x.columns, **kwargs)\n",
    "    \n",
    "    return root\n",
    "\n",
    "\n",
    "def predict_tree(root, newdata, **kwargs):\n",
    "    return test_predictions(root, newdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EC1PVHpyg7jG"
   },
   "outputs": [],
   "source": [
    "def upliftRF(x, y, t, ntree=10, bagging_fraction=0.6, random_seed=1234, **kwargs):\n",
    "    predict_attr = kwargs.get('predict_attr', 'Y')\n",
    "    treatment_attr = kwargs.get('treatment_attr', 'T')\n",
    "    \n",
    "    df = x.copy()\n",
    "    df[predict_attr] = y\n",
    "    df[treatment_attr] = t\n",
    "    \n",
    "    kwargs['predict_attr'] = predict_attr\n",
    "    kwargs['treatment_attr'] = treatment_attr\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    random_seeds = [np.random.randint(10000) for _ in range(ntree)]\n",
    "    trees = []\n",
    "    for i in range(ntree):\n",
    "        bagged_df = df.sample(frac=bagging_fraction, random_state=random_seeds[i])\n",
    "        trees.append(build_tree(bagged_df, x.columns, random_seed=random_seeds[i], **kwargs))\n",
    "    \n",
    "    return trees\n",
    "\n",
    "\n",
    "def predict_upliftRF(obj, newdata, **kwargs):\n",
    "    pred_trees = []\n",
    "    for tree in obj:\n",
    "        pred_trees.append(test_predictions(tree, newdata))\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"pr_y1_t1\": sum([x['pr_y1_t1'] for x in pred_trees])/len(pred_trees),\n",
    "        \"pr_y1_t0\": sum([x['pr_y1_t0'] for x in pred_trees])/len(pred_trees),\n",
    "    })\n",
    "    return pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNPe9sFyW0T4"
   },
   "outputs": [],
   "source": [
    "def upliftRF_ed(*args, **kwargs):\n",
    "    kwargs['method'] = 'ed'\n",
    "    return upliftRF(*args, **kwargs)\n",
    "  \n",
    "def upliftRF_kl(*args, **kwargs):\n",
    "    kwargs['method'] = 'kl'\n",
    "    return upliftRF(*args, **kwargs)\n",
    "  \n",
    "def upliftRF_chi(*args, **kwargs):\n",
    "    kwargs['method'] = 'chisq'\n",
    "    return upliftRF(*args, **kwargs)\n",
    "  \n",
    "def upliftRF_int(*args, **kwargs):\n",
    "    kwargs['method'] = 'int'\n",
    "    return upliftRF(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOPp6WQ7rFNF"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def z_assign(y, t):\n",
    "    \"\"\" Define transformed response variable z\n",
    "    if (treated and response) or (not treated and not response), return 1\n",
    "    else, return 0\n",
    "    \"\"\"\n",
    "    if y == 1 and t == 1:\n",
    "        return 1\n",
    "    elif y == 0 and t == 1:\n",
    "        return 0\n",
    "    elif y == 1 and t == 0:\n",
    "        return 0\n",
    "    elif y == 0 and t == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lai(x, y, t, method=GradientBoostingClassifier, **kwargs):\n",
    "    \"\"\"Training a model according to the \"Lai's Approach\" \n",
    "    The default model is Gradient Boosting Machine (gbm)\n",
    "\n",
    "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
    "            Customers Using True Lift Overview\" (Kane, 2014)\n",
    "\n",
    "    Args:\n",
    "        x: A data frame of predictors.\n",
    "        y: A binary response (numeric) vector.\n",
    "        t: A binary response (numeric) representing the treatment assignment\n",
    "            (coded as 0/1).\n",
    "        method: A sklearn model specifying which classification or regression\n",
    "            model to use. This should be a method that can handle a \n",
    "            multinominal class variable.\n",
    "\n",
    "    Return:\n",
    "        A sklearn model.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'y': y.copy()})\n",
    "    df['t'] = t\n",
    "    z = df.apply(lambda row: z_assign(row['y'], row['t']), axis=1)\n",
    "    \n",
    "    model = method(**kwargs).fit(x, z)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_lai(obj, newdata, **kwargs):\n",
    "    \"\"\"Predictions according to the \"Lai's Approach\" \n",
    "    \n",
    "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
    "            Customers Using True Lift Overview\" (Kane, 2014)\n",
    "\n",
    "    Args:\n",
    "        obj: A sklearn model.\n",
    "        newdata: A data frame containing the values at which predictions\n",
    "            are required.\n",
    "    \n",
    "    Return:\n",
    "        dataframe: A dataframe with predictions for when the instances are\n",
    "            treated and for when they are not treated.\n",
    "    \"\"\"\n",
    "    pred = obj.predict_proba(newdata)    # list of [False, True]\n",
    "\n",
    "    res = pd.DataFrame({\n",
    "        \"pr_y1_t1\": [row[1] for row in pred],\n",
    "        \"pr_y1_t0\": [row[0] for row in pred],\n",
    "    })\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0psF5SuwgWP"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def glai(x, y, t, method=GradientBoostingClassifier, **kwargs):\n",
    "    \"\"\"Training a model according to the \"Lai's Approach\" \n",
    "    The default model is Gradient Boosting Machine (gbm)\n",
    "\n",
    "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
    "            Customers Using True Lift Overview\" (Kane, 2014)\n",
    "\n",
    "    Args:\n",
    "        x: A data frame of predictors.\n",
    "        y: A binary response (numeric) vector.\n",
    "        t: A binary response (numeric) representing the treatment assignment\n",
    "            (coded as 0/1).\n",
    "        method: A sklearn model specifying which classification or regression\n",
    "            model to use. This should be a method that can handle a \n",
    "            multinominal class variable.\n",
    "\n",
    "    Return:\n",
    "        A sklearn model.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'y': y.copy()})\n",
    "    df['t'] = t\n",
    "    ty = df.apply(lambda row: ty_assign(row['y'], row['t']), axis=1)\n",
    "    \n",
    "    model = method(**kwargs).fit(x, ty)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_glai(obj, newdata, y, t, **kwargs):\n",
    "    \"\"\"Predictions according to the \"Lai's Approach\" \n",
    "    \n",
    "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
    "            Customers Using True Lift Overview\" (Kane, 2014)\n",
    "\n",
    "    Args:\n",
    "        obj: A sklearn model.\n",
    "        newdata: A data frame containing the values at which predictions\n",
    "            are required.\n",
    "    \n",
    "    Return:\n",
    "        dataframe: A dataframe with predictions for when the instances are\n",
    "            treated and for when they are not treated.\n",
    "    \"\"\"\n",
    "    prob_T = sum(t) / len(t)\n",
    "    prob_C = 1 - prob_T\n",
    "\n",
    "    pred = obj.predict_proba(newdata)    # list of [CN, CR, TN, TR]\n",
    "\n",
    "    res = pd.DataFrame({\n",
    "        \"pr_y1_t1\": [row[3]/prob_T + row[0]/prob_C for row in pred],   # TR/T + CN/C\n",
    "        \"pr_y1_t0\": [row[2]/prob_T + row[1]/prob_C for row in pred],   # TN/T + CR/C\n",
    "    })\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqztMu4voFLV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def z_assign(y, t):\n",
    "    \"\"\" Define transformed response variable z\n",
    "    if (treated and response) or (not treated and not response), return 1\n",
    "    else, return 0\n",
    "    \"\"\"\n",
    "    if y == 1 and t == 1:\n",
    "        return 1\n",
    "    elif y == 0 and t == 1:\n",
    "        return 0\n",
    "    elif y == 1 and t == 0:\n",
    "        return 0\n",
    "    elif y == 0 and t == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rvtu(x, y, t, method=LogisticRegression, **kwargs):\n",
    "    \"\"\"Transforming the data according to the \"Jaskowski's Approach\"\n",
    "    Sometimes, it called Response Variable Transformation for Uplift (RVTU)\n",
    "\n",
    "    Source: \"Uplift modeling for clinical trial data\" (Jaskowski, 2006)\n",
    "    \"\"\"\n",
    "\n",
    "    ### Combine x, y, and ct\n",
    "    df = x.copy()\n",
    "    df['y'] = y\n",
    "    df['ct'] = t\n",
    "    df['z'] = df.apply(lambda row: z_assign(row['y'], row['ct']), axis=1)\n",
    " \n",
    "    mdl = method(**kwargs).fit(x, df['z'])\n",
    "    \n",
    "    return mdl\n",
    "\n",
    "\n",
    "def predict_rvtu(obj, newdata, y, t, **kwargs):\n",
    "    # df = pd.DataFrame({'y': y.copy()})\n",
    "    # df['ct'] = ct\n",
    "    # z = df.apply(lambda row: z_assign(row['y'], row['ct']), axis=1)\n",
    "\n",
    "    if isinstance(obj, LinearRegression):\n",
    "        pred = obj.predict(newdata)\n",
    "    else:\n",
    "        pred = obj.predict_proba(newdata)[:, 1]\n",
    "\n",
    "    res = pd.DataFrame({\n",
    "        \"pr_y1_t1\": [row for row in pred],\n",
    "        \"pr_y1_t0\": [1-row for row in pred],\n",
    "    })\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vK0a6vs9ss3R"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def dta(x, y, t, method=LogisticRegression, **kwargs):\n",
    "    \"\"\"Training a model according to the \"Dummy Treatment Approach\" \n",
    "    The default model is General Linear Model (GLM)\n",
    "\n",
    "    Source: \"The True Lift Model\" (Lo, 2002)\n",
    "\n",
    "    Args:\n",
    "        x: A data frame of predictors.\n",
    "        y: A binary response (numeric) vector.\n",
    "        t: A binary response (numeric) representing the treatment assignment\n",
    "            (coded as 0/1).\n",
    "        method: A sklearn model specifying which classification or regression\n",
    "            model to use. This should be a method that can handle a \n",
    "            multinominal class variable.\n",
    "\n",
    "    Return:\n",
    "        A sklearn model.\n",
    "    \"\"\"\n",
    "    # Create interaction variables\n",
    "    # Building our dataframe with the interaction variables\n",
    "    df = x.copy()\n",
    "    for colname in x.columns:\n",
    "        df[\"Int_\" + colname] = x[colname] * t\n",
    "    df['treated'] = t\n",
    "\n",
    "    # Fit a model\n",
    "    model = method(**kwargs).fit(df, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_dta(obj, newdata, y_name='y', t_name='treated', **kwargs):\n",
    "    \"\"\"Predictions according to the \"Dummy Treatment Approach\" \n",
    "    \n",
    "    For each instance in newdata two predictions are made:\n",
    "    1) What is the probability of a person responding when treated?\n",
    "    2) What is the probability of a person responding when not treated\n",
    "      (i.e. part of control group)?\n",
    "\n",
    "    Source: \"The True Lift Model\" (Lo, 2002)\n",
    "\n",
    "    Args:\n",
    "        obj: A sklearn model.\n",
    "        newdata: A data frame containing the values at which predictions\n",
    "            are required.\n",
    "    \n",
    "    Return:\n",
    "        dataframe: A dataframe with predictions for when the instances are\n",
    "            treated and for when they are not treated.\n",
    "    \"\"\"\n",
    "    predictors = [c for c in newdata.columns if c not in (y_name, t_name)]\n",
    "\n",
    "    df_treat = newdata.copy()\n",
    "    df_control = newdata.copy()\n",
    "    for colname in predictors:\n",
    "        df_treat[\"Int_\" + colname] = df_treat[colname] * 1\n",
    "        df_control[\"Int_\" + colname] = df_control[colname] * 0\n",
    "    df_treat['treated'] = 1\n",
    "    df_control['treated'] = 0\n",
    "\n",
    "    # print(obj.coef_, obj.intercept_)\n",
    "    pred_treat = obj.predict_proba(df_treat)[:, 1]\n",
    "    pred_control = obj.predict_proba(df_control)[:, 1]\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"pr_y1_t1\": pred_treat,\n",
    "        \"pr_y1_t0\": pred_control,\n",
    "    })\n",
    "    return pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uj3olZCN_-ML",
    "outputId": "cc238f8c-25e1-4fd6-ad3b-7c3adafe650b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Load data ###\n",
    "df = pd.read_csv('data/Hillstrom.csv')\n",
    "dataset = 'hillstrom'\n",
    "# df = pd.read_csv('criteo_small.csv')\n",
    "# dataset = 'criteo'\n",
    "# df = pd.read_csv('Lalonde.csv')\n",
    "# dataset = 'lalonde'\n",
    "df = preprocess_data(df, dataset=dataset)\n",
    "Y = df['Y']\n",
    "T = df['T']\n",
    "X = df.drop(['Y', 'T'], axis=1)\n",
    "ty = pd.DataFrame({'Y': Y, 'T': T})\\\n",
    "         .apply(lambda row: ty_assign(row['Y'], row['T']), axis=1)  \n",
    "  \n",
    "### Experiment procedure ###\n",
    "rf_params = {\n",
    "    'ntree': [10, 100],\n",
    "    'mtry': [3, ],\n",
    "    'bagging_fraction': [0.6, ],\n",
    "    'max_depth': [3, 5,],\n",
    "    'min_split': [1000,],\n",
    "    'min_bucket_t0': [100,],\n",
    "    'min_bucket_t1': [100,],\n",
    "#     'random_seed': [1234],\n",
    "}  #=4\n",
    "\n",
    "lr_params = {\n",
    "    'method': [LogisticRegression],\n",
    "    'solver': ['newton-cg',], # 'lbfgs', 'sag', 'saga'],\n",
    "    'penalty': ['none', 'l2'],\n",
    "    'tol': [1e-2, 1e-3, 1e-4],\n",
    "    'C': [1e6, 1e3, 1, 1e-3, 1e-6],\n",
    "}  #=30\n",
    "# lr_params = {\n",
    "#     'solver': ['liblinear', 'newton-cg'],\n",
    "# }\n",
    "# method = tma\n",
    "# predict_method = predict_tma\n",
    "\n",
    "\n",
    "sgb_params = {\n",
    "    'learning_rate': [0.1,],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3,],\n",
    "    'random_state': [1234],\n",
    "    'max_features': [None, 'sqrt']\n",
    "}  #=24\n",
    "method_funcs = [\n",
    "    [tma, predict_tma, lr_params],\n",
    "    [dta, predict_dta, lr_params],\n",
    "    [lai, predict_lai, sgb_params],\n",
    "    [glai, predict_glai, sgb_params],\n",
    "    [rvtu, predict_rvtu, lr_params],\n",
    "    [upliftRF_ed, predict_upliftRF, rf_params],\n",
    "    [upliftRF_kl, predict_upliftRF, rf_params],\n",
    "    [upliftRF_chi, predict_upliftRF, rf_params],\n",
    "    [upliftRF_int, predict_upliftRF, rf_params],\n",
    "]\n",
    "all_q_list = {}\n",
    "all_wrapped_qini = {}\n",
    "all_perfs = {}\n",
    "all_preds = {}\n",
    "for method, predict_method, search_space in method_funcs:\n",
    "    if dataset == 'lalonde':\n",
    "        fold_gen = KFold(n_splits=5, shuffle=True, random_state=1234).split(X)\n",
    "    else:\n",
    "        fold_gen = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234).split(X, ty)\n",
    "    q_list = []\n",
    "    wrapped_qini_list = []\n",
    "    perf_list = []\n",
    "    pred_list = []\n",
    "    for idx, (train_index, test_index) in enumerate(fold_gen):\n",
    "        X_train = X.reindex(train_index)\n",
    "        X_test = X.reindex(test_index)\n",
    "        Y_train = Y.reindex(train_index)\n",
    "        Y_test = Y.reindex(test_index)\n",
    "        T_train = T.reindex(train_index)\n",
    "        T_test = T.reindex(test_index)\n",
    "#         min_max_scaler = MinMaxScaler()\n",
    "#         fitted = min_max_scaler.fit(X_train)\n",
    "#         X_train = min_max_scaler.transform(X_train)\n",
    "#         X_test = min_max_scaler.transform(X_train)\n",
    "\n",
    "        df = X_train.copy()\n",
    "        df['Y'] = Y_train\n",
    "        df['T'] = T_train\n",
    "        if dataset == 'lalonde':\n",
    "            stratify = T_train\n",
    "        else:\n",
    "            stratify = df[['Y', 'T']]\n",
    "        tuning_df, validate_df = train_test_split(\n",
    "            df, test_size=0.33, random_state=1234, stratify=stratify)\n",
    "\n",
    "        X_tuning = tuning_df.drop(['Y', 'T'], axis=1)\n",
    "        Y_tuning = tuning_df['Y']\n",
    "        T_tuning = tuning_df['T']\n",
    "        X_validate = validate_df.drop(['Y', 'T'], axis=1)\n",
    "        Y_validate = validate_df['Y']\n",
    "        T_validate = validate_df['T']\n",
    "        data_dict = {\n",
    "            \"x_train\": X_tuning,\n",
    "            \"y_train\": Y_tuning,\n",
    "            \"t_train\": T_tuning,\n",
    "            \"x_test\": X_validate,\n",
    "            \"y_test\": Y_validate,\n",
    "            \"t_test\": T_validate,\n",
    "        }\n",
    "\n",
    "        ### Variable Selection: General wrapper approach ###\n",
    "        if search_space == lr_params:\n",
    "            params = {k:v[0] for k, v in search_space.items()}\n",
    "\n",
    "            _, drop_vars, qini_values = wrapper(\n",
    "                    method, predict_method, data_dict, params=params)\n",
    "            wrapped_qini_list.append(qini_values)\n",
    "            best_qini = max(qini_values)\n",
    "            best_idx = qini_values.index(best_qini)\n",
    "            best_drop_vars = drop_vars[:best_idx]\n",
    "\n",
    "            X_tuning.drop(best_drop_vars, axis=1, inplace=True)\n",
    "            X_validate.drop(best_drop_vars, axis=1, inplace=True)\n",
    "            X_train.drop(best_drop_vars, axis=1, inplace=True)\n",
    "            X_test.drop(best_drop_vars, axis=1, inplace=True)\n",
    "\n",
    "        ### Hyperparameters tuning ###\n",
    "        _, best_params = parameter_tuning(method, predict_method, data_dict, \n",
    "                                          search_space=search_space, plotit=False)\n",
    "\n",
    "        print(\"Best_params: \", best_params)\n",
    "        trained_model = method(X_train, Y_train, T_train, **best_params)\n",
    "        pred = predict_method(trained_model, X_test, y=Y_test, t=T_test)\n",
    "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], Y_test, T_test)\n",
    "        q = qini(perf, plotit=False)\n",
    "        q_list.append(q)\n",
    "        perf_list.append(perf)\n",
    "        pred_list.append(pred)\n",
    "    all_preds[method] = pred_list\n",
    "    all_perfs[method] = perf_list\n",
    "    all_q_list[method] = q_list\n",
    "    print(\"Method: {}\".format(method))\n",
    "    print(\"search space:\", search_space)\n",
    "    qini_list = [q['qini'] for q in q_list]\n",
    "    print('Qini values: ', qini_list)\n",
    "    print('    mean: {}, std: {}'.format(np.mean(qini_list), np.std(qini_list)))\n",
    "\n",
    "    wrapped_qini = np.array(wrapped_qini_list)\n",
    "    all_wrapped_qini[method] = wrapped_qini\n",
    "    if search_space == lr_params:\n",
    "        wrapped_qini = wrapped_qini.mean(axis=0)[::-1]\n",
    "        plt.plot(np.arange(1, len(wrapped_qini)+1), wrapped_qini*100)\n",
    "        plt.show()\n",
    "\n",
    "    x_axis = np.arange(0, 1.1, 0.1)\n",
    "    qini_curve = np.array([q['inc_gains'] for q in q_list]).mean(axis=0)\n",
    "    qini_curve_rand = np.array([q['random_inc_gains'] for q in q_list]).mean(axis=0)\n",
    "    plt.plot(x_axis, qini_curve*100)\n",
    "    plt.plot(x_axis, qini_curve_rand*100)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFHGmBll7Ovs"
   },
   "outputs": [],
   "source": [
    "hillstrom_all_q_list = all_q_list\n",
    "hillstrom_all_wrapped_qini = all_wrapped_qini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6zKPjZXOvrMN",
    "outputId": "5ce83af0-2beb-4294-effb-c3987770aa33"
   },
   "outputs": [],
   "source": [
    "# hillstrom_all_q_list: dict, keys are function & values are:\n",
    "#     -> list of 5 dicts which are return of the qini function\n",
    "#        keys are 'qini', 'inc_gains', 'random_inc_gains', 'auuc_list', 'auuc_rand_list', 'qini_30p', 'qini_10p'\n",
    "print(\"Hillstrom: General wrapper approach\")\n",
    "key = list(hillstrom_all_wrapped_qini.keys())[0]\n",
    "x_axis = np.arange(1, len(hillstrom_all_wrapped_qini[key][0])+1)\n",
    "for key, value in hillstrom_all_wrapped_qini.items():\n",
    "    if len(value) == 0:\n",
    "        continue\n",
    "    curve = np.array(value).mean(axis=0)[::-1]\n",
    "    plt.plot(x_axis, curve*100, label=key.__name__)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Hillstrom: Qini values\")\n",
    "x_axis = np.arange(0, 1.1, 0.1)\n",
    "for key, value in hillstrom_all_q_list.items():\n",
    "    qini_curve = np.array([q['inc_gains'] for q in value]).mean(axis=0)\n",
    "    plt.plot(x_axis, qini_curve*100, label=key.__name__)\n",
    "qini_curve_rand = np.array([q['random_inc_gains'] for q in value]).mean(axis=0)\n",
    "plt.plot(x_axis, qini_curve_rand*100, label=\"Random\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Fold 1\")\n",
    "x_axis = np.arange(0, 1.1, 0.1)\n",
    "for key, value in hillstrom_all_q_list.items():\n",
    "    qini_curve = np.array(value[0]['inc_gains'])\n",
    "    plt.plot(x_axis, qini_curve*100, label=key.__name__)\n",
    "qini_curve_rand = np.array(value[0]['random_inc_gains'])\n",
    "plt.plot(x_axis, qini_curve_rand*100, label=\"Random\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Fold 3\")\n",
    "x_axis = np.arange(0, 1.1, 0.1)\n",
    "for key, value in hillstrom_all_q_list.items():\n",
    "    qini_curve = np.array(value[2]['inc_gains'])\n",
    "    plt.plot(x_axis, qini_curve*100, label=key.__name__)\n",
    "qini_curve_rand = np.array(value[2]['random_inc_gains'])\n",
    "plt.plot(x_axis, qini_curve_rand*100, label=\"Random\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "qini_values = {}\n",
    "inv_qini = {}\n",
    "for key, value in hillstrom_all_q_list.items():\n",
    "    mean_qini = np.array([q['qini'] for q in value]).mean(axis=0)\n",
    "    qini_values[key] = mean_qini\n",
    "    inv_qini[mean_qini] = inv_qini.get(mean_qini, []) + [key]\n",
    "max_qini = max(qini_values.values())\n",
    "min_qini = min(qini_values.values())\n",
    "print(\"  MAX: \", [f.__name__ for f in inv_qini[max_qini]])\n",
    "print(\"  MIN: \", [f.__name__ for f in inv_qini[min_qini]])\n",
    "if len(inv_qini[max_qini]) != 1 or len(inv_qini[min_qini]) != 1:\n",
    "    print(\"The method with MAX or MIN qini value is not only one!!\")\n",
    "best_method = inv_qini[max_qini][0]\n",
    "worst_method = inv_qini[min_qini][0]\n",
    "for idx, q in enumerate(hillstrom_all_q_list[best_method]):\n",
    "    plt.plot(x_axis, np.array(q['inc_gains'])*100, label=\"Fold {}\".format(idx+1))\n",
    "qini_curve_rand = np.array([q['random_inc_gains'] for q in hillstrom_all_q_list[best_method]]).mean(axis=0)\n",
    "plt.plot(x_axis, qini_curve_rand*100, label='Random')\n",
    "print('Hillstrom - {}'.format(best_method.__name__))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "for idx, q in enumerate(hillstrom_all_q_list[worst_method]):\n",
    "    plt.plot(x_axis, np.array(q['inc_gains'])*100, label=\"Fold {}\".format(idx+1))\n",
    "qini_curve_rand = np.array([q['random_inc_gains'] for q in hillstrom_all_q_list[worst_method]]).mean(axis=0)\n",
    "plt.plot(x_axis, qini_curve_rand*100, label='Random')\n",
    "print('Hillstrom - {}'.format(worst_method.__name__))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for fname in list(hillstrom_all_q_list.keys()):\n",
    "    qini_list = [q['qini']*100 for q in hillstrom_all_q_list[fname]]\n",
    "    print('{0:>15}: {1:<7} ({2:<7})'.format(fname.__name__, round(np.mean(qini_list), 5), round(np.std(qini_list), 5),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "code_day4_hillstrom.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
