{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehi1150I7fxN"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_hilstrom = 'https://drive.google.com/open?id=15osyN4c5z1pSo1JkxwL_N8bZTksRvQuU'\n",
    "fluff, id = link_hilstrom.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('Hillstrom.csv')\n",
    "hillstrom_df = pd.read_csv('Hillstrom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tAS92JMPe9U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_lalonde = 'https://drive.google.com/open?id=1b8N7WtwIe2WmQJD1KL5UAy70K13MxwKj'\n",
    "fluff, id = link_lalonde.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('Lalonde.csv')\n",
    "lalonde_df = pd.read_csv('Lalonde.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "ZoNrZI5P80wJ",
    "outputId": "88345ba3-d1bc-47e4-c3d7-0686b1bed600"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "def preprocess_data(df, dataset='hillstrom', verbose=True):\n",
    "    # For Hillstrom dataset, the ‘‘visit’’ target variable was selected\n",
    "    #   as the target variable of interest and the selected treatment is \n",
    "    #   the e-mail campaign for women’s merchandise [1]\n",
    "    # [1] Kane K, Lo VSY, Zheng J. True-lift modeling: Comparison of methods. \n",
    "    #    J Market Anal. 2014;2:218–238\n",
    "    dataset = dataset.lower()\n",
    "    if dataset in ('hillstrom', 'email'):\n",
    "        columns = df.columns\n",
    "        for col in columns:\n",
    "            if df[col].dtype != object:\n",
    "                continue\n",
    "            df = pd.concat(\n",
    "                    [df, pd.get_dummies(df[col], \n",
    "                                        prefix=col, \n",
    "                                        drop_first=False)],\n",
    "                    axis=1)\n",
    "            df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        df.columns = [col.replace('-', '').replace(' ', '_').lower()\n",
    "                      for col in df.columns]\n",
    "        df = df[df.segment_mens_email == 0]\n",
    "        df.index = range(len(df))\n",
    "        df.drop(['segment_mens_email', \n",
    "                 'segment_no_email', \n",
    "                 'conversion', \n",
    "                 'spend'], axis=1, inplace=True)\n",
    "\n",
    "        y_name = 'visit'\n",
    "        t_name = 'segment_womens_email'\n",
    "    elif dataset in ['criteo', 'ad']:\n",
    "        raise NotImplementedError\n",
    "    elif dataset in ['lalonde', 'job']:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    df['Y'] = df[y_name]\n",
    "    df.drop([y_name], axis=1, inplace=True)\n",
    "    df['T'] = df[t_name]\n",
    "    df.drop([t_name], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Igf3QLgdJ1cW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def performance(pr_y1_t1, pr_y1_t0, y, t, groups=10):\n",
    "    \"\"\"\n",
    "    1. Split the total customers into the given number of groups\n",
    "    2. Calculate the statistics of each segment\n",
    "    \n",
    "    Args:\n",
    "        pr_y1_t1: the series (list) of the customer's expected return\n",
    "        pr_y1_t0: the expected return when a customer is not treated\n",
    "        y: the observed return of customers\n",
    "        t: whther each customer is treated or not\n",
    "        groups: the number of groups (segments). Should be 5, 10, or 20\n",
    "    Return:\n",
    "        DataFrame:\n",
    "            columns:\n",
    "                'n_y1_t1': the number of treated responders\n",
    "                'n_y1_t0': the number of not treated responders\n",
    "                'r_y1_t1': the average return of treated customers\n",
    "                'r_y1_t0': the average return of not treated customers\n",
    "                'n_t1': the number of treated customers\n",
    "                'n_t0': the number of not treated customers\n",
    "                'uplift': the average uplift (the average treatment effect)\n",
    "            rows: the index of groups\n",
    "    \"\"\"\n",
    "  \n",
    "    ### check valid arguments\n",
    "    if groups not in [5, 10, 20]:\n",
    "        raise Exception(\"uplift: groups must be either 5, 10 or 20\")\n",
    "  \n",
    "    ### check for NAs.\n",
    "    if pr_y1_t1.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in pr_y1_t1\")\n",
    "    if pr_y1_t0.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in pr_y1_t0\")\n",
    "    if y.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in y\")\n",
    "    if t.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in t\")\n",
    "   \n",
    "    ### check valid values for y and t\n",
    "    # if set(y) != {0, 1}:\n",
    "    #     raise Exception(\"uplift: y must be either 0 or 1\")\n",
    "    if set(t) != {0, 1}:\n",
    "        raise Exception(\"uplift: t must be either 0 or 1\")\n",
    "\n",
    "    ### check length of arguments\n",
    "    if not (len(pr_y1_t1) == len(pr_y1_t0) == len(y) == len(t)):\n",
    "        raise Exception(\"uplift: arguments pr_y1_t1, pr_y1_t0, y and t must all have the same length\")\n",
    "\n",
    "    ### define dif_pred\n",
    "    dif_pred = pr_y1_t1 - pr_y1_t0\n",
    "  \n",
    "    ### Make index same\n",
    "    y.index = dif_pred.index\n",
    "    t.index = dif_pred.index\n",
    "    \n",
    "    mm = pd.DataFrame({\n",
    "        'dif_pred': dif_pred,\n",
    "        'y': y,\n",
    "        't': t,\n",
    "        'dif_pred_r': dif_pred.rank(ascending=False, method='first')\n",
    "    })\n",
    "\n",
    "    mm_groupby = mm.groupby(pd.qcut(mm['dif_pred_r'], groups, labels=range(1, groups+1), duplicates='drop'))\n",
    "  \n",
    "    n_y1_t1 = mm_groupby.apply(lambda r: r[r['t'] == 1]['y'].sum())\n",
    "    n_y1_t0 = mm_groupby.apply(lambda r: r[r['t'] == 0]['y'].sum())\n",
    "    n_t1 = mm_groupby['t'].sum()\n",
    "    n_t0 = mm_groupby['t'].count() - n_t1\n",
    "  \n",
    "    df = pd.DataFrame({\n",
    "        'n_t1': n_t1,\n",
    "        'n_t0': n_t0,\n",
    "        'n_y1_t1': n_y1_t1,\n",
    "        'n_y1_t0': n_y1_t0,\n",
    "        'r_y1_t1': n_y1_t1 / n_t1,\n",
    "        'r_y1_t0': n_y1_t0 / n_t0,\n",
    "    })\n",
    "    fillna_columns = ['n_y1_t1', 'n_y1_t0', 'n_t1', 'n_t0']\n",
    "    df[fillna_columns] = df[fillna_columns].fillna(0)\n",
    "    df.index.name = 'groups'\n",
    "\n",
    "    df['uplift'] = df['r_y1_t1'] - df['r_y1_t0']\n",
    "    df['uplift'] = round(df['uplift'], 6)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def qini(perf, plotit=True):\n",
    "    nrow = len(perf)\n",
    "\n",
    "    # Calculating the incremental gains. \n",
    "    # - First, the cumulitative sum of the treated and the control groups are\n",
    "    #  calculated with respect to the total population in each group at the\n",
    "    #  specified decile\n",
    "    # - Afterwards we calculate the percentage of the total amount of people\n",
    "    #  (both treatment and control) are present in each decile\n",
    "    cumul_y1_t1 = (perf['n_y1_t1'].cumsum() / perf['n_t1'].cumsum()).fillna(0)\n",
    "    cumul_y1_t0 = (perf['n_y1_t0'].cumsum() / perf['n_t0'].cumsum()).fillna(0)\n",
    "    deciles = [i/nrow for i in range(1, nrow+1)]\n",
    "\n",
    "    ### Model Incremental gains\n",
    "    inc_gains = (cumul_y1_t1 - cumul_y1_t0) * deciles\n",
    "    inc_gains = [0.0] + list(inc_gains)\n",
    "\n",
    "    ### Overall incremental gains\n",
    "    overall_inc_gain = sum(perf['n_y1_t1']) / sum(perf['n_t1']) \\\n",
    "            - sum(perf['n_y1_t0']) / sum(perf['n_t0'])\n",
    "\n",
    "    ### Random incremental gains\n",
    "    random_inc_gains = [i*overall_inc_gain / nrow for i in range(nrow+1)]\n",
    "\n",
    "    ### Compute area under the model incremental gains (uplift) curve\n",
    "    x = [0] + deciles\n",
    "    y = list(inc_gains)\n",
    "    auuc = 0\n",
    "    auuc_rand = 0\n",
    "\n",
    "    auuc_list = [auuc]\n",
    "    for i in range(1, len(x)):\n",
    "        auuc += 0.5 * (x[i] - x[i-1]) * (y[i] + y[i-1])\n",
    "        auuc_list.append(auuc)\n",
    "\n",
    "    ### Compute area under the random incremental gains curve\n",
    "    y_rand = random_inc_gains\n",
    "\n",
    "    auuc_rand_list = [auuc_rand]\n",
    "    for i in range(1, len(x)):\n",
    "        auuc_rand += 0.5 * (x[i] - x[i-1]) * (y_rand[i] + y_rand[i-1])\n",
    "        auuc_rand_list.append(auuc_rand)\n",
    "\n",
    "    ### Compute the difference between the areas (Qini coefficient)\n",
    "    Qini = auuc - auuc_rand\n",
    "\n",
    "    ### Plot incremental gains curve\n",
    "    if plotit:\n",
    "        x_axis = x\n",
    "        plt.plot(x_axis, inc_gains)\n",
    "        plt.plot(x_axis, random_inc_gains)\n",
    "        plt.show()\n",
    "    \n",
    "    ### Qini 30%, Qini 10%\n",
    "    n_30p = int(nrow*3/10)\n",
    "    n_10p = int(nrow/10)\n",
    "    qini_30p = auuc_list[n_30p] - auuc_rand_list[n_30p]\n",
    "    qini_10p = auuc_list[n_10p] - auuc_rand_list[n_10p]\n",
    "\n",
    "    res = {\n",
    "        'qini': Qini,\n",
    "        'inc_gains': inc_gains,\n",
    "        'random_inc_gains': random_inc_gains,\n",
    "        'auuc_list': auuc_list,\n",
    "        'auuc_rand_list': auuc_rand_list,\n",
    "        'qini_30p': qini_30p,\n",
    "        'qini_10p': qini_10p,\n",
    "    }    \n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nrxjl1v7J9Mm"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def parameter_tuning(fit_mdl, pred_mdl, data, search_space):\n",
    "    \"\"\"\n",
    "    Given a model, search all combination of parameter sets and find\n",
    "    the best parameter set\n",
    "    \n",
    "    Args:\n",
    "        fit_mdl: model function\n",
    "        pred_mdl: predict function of fit_mdl\n",
    "        data:\n",
    "            {\n",
    "                \"x_train\": predictor variables of training dataset,\n",
    "                \"y_train\": target variables of training dataset,\n",
    "                \"t_train\": treatment variables of training dataset,\n",
    "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
    "                \"y_test\": target variables of test (usually, validation) dataset,\n",
    "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
    "            }\n",
    "        search_space:\n",
    "            {\n",
    "                parameter_name: [search values]\n",
    "            }\n",
    "    Return:\n",
    "        The best parameter set\n",
    "    \"\"\"\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    t_train = data['t_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "    t_test = data['t_test']\n",
    "    \n",
    "    max_q = -float('inf')\n",
    "    best_mdl = None\n",
    "\n",
    "    keys = search_space.keys()\n",
    "    n_space = [len(search_space[key]) for key in keys]\n",
    "    n_iter = np.prod(n_space)\n",
    "    \n",
    "    best_params = None\n",
    "    for i in range(n_iter):\n",
    "        params = {}\n",
    "        for idx, key in enumerate(keys):\n",
    "            params[key] = search_space[key][i % n_space[idx]]\n",
    "            i = int(i / n_space[idx])\n",
    "\n",
    "        mdl = fit_mdl(x_train, y_train, t_train, **params)\n",
    "        pred = pred_mdl(mdl, newdata=x_test, y=y_test, ct=t_test)\n",
    "        # print('    {}'.format(params))\n",
    "        try:\n",
    "            perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        q = qini(perf, plotit=False)['qini']\n",
    "        if q > max_q:\n",
    "            max_q = q\n",
    "            best_mdl = mdl\n",
    "            best_params = params\n",
    "\n",
    "    return best_mdl, best_params\n",
    "\n",
    "\n",
    "def wrapper(fit_mdl, pred_mdl, data, params=None,\n",
    "            best_models=None, drop_variables=None, qini_values=None):\n",
    "    \"\"\"\n",
    "    General wrapper approach\n",
    "    \n",
    "    Args:\n",
    "        fit_mdl: model function\n",
    "        pred_mdl: predict function of fit_mdl\n",
    "        data:\n",
    "            {\n",
    "                \"x_train\": predictor variables of training dataset,\n",
    "                \"y_train\": target variables of training dataset,\n",
    "                \"t_train\": treatment variables of training dataset,\n",
    "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
    "                \"y_test\": target variables of test (usually, validation) dataset,\n",
    "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
    "            }\n",
    "    Return:\n",
    "        (A list of best models, The list of dropped variables)\n",
    "    \"\"\"\n",
    "    if best_models is None:\n",
    "        best_models = []\n",
    "    if drop_variables is None:\n",
    "        drop_variables = []\n",
    "    if qini_values is None:\n",
    "        qini_values = []\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    t_train = data['t_train']\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "    t_test = data['t_test']\n",
    "\n",
    "    variables = data['x_train'].columns\n",
    "\n",
    "    max_q = -float('inf')\n",
    "    drop_var = None\n",
    "    best_mdl = None\n",
    "    for var in variables:\n",
    "        if var in drop_variables:\n",
    "            continue\n",
    "        x = x_train.copy()\n",
    "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
    "        mdl = fit_mdl(x, y_train, t_train, **params)\n",
    "        x = x_test.copy()\n",
    "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
    "        pred = pred_mdl(mdl, newdata=x, y=y_test, ct=t_test)\n",
    "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
    "        q = qini(perf, plotit=False)['qini']\n",
    "        if q > max_q:\n",
    "            max_q = q\n",
    "            drop_var = var\n",
    "            best_mdl = mdl\n",
    "    \n",
    "    best_models.append(best_mdl)\n",
    "    drop_variables.append(drop_var)\n",
    "    qini_values.append(max_q)\n",
    "\n",
    "    left_vars = [var for var in variables if (var not in drop_variables)]\n",
    "    \n",
    "    if len(variables) == len(drop_variables) + 1:\n",
    "        return best_models, drop_variables + left_vars, qini_values\n",
    "    else:\n",
    "        return wrapper(fit_mdl, pred_mdl, data, params=params,\n",
    "                       best_models=best_models, drop_variables=drop_variables,\n",
    "                       qini_values=qini_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxrEbEODlbQi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def tma(x, y, t, method=LogisticRegression, **kwargs):\n",
    "    \"\"\"Training a model according to the \"Two Model Approach\" \n",
    "    (a.k.a. \"Separate Model Approach\")\n",
    "    The default model is General Linear Model (GLM)\n",
    "    \n",
    "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
    "\n",
    "    Args:\n",
    "        x: A data frame of predictors.\n",
    "        y: A binary response (numeric) vector.\n",
    "        t: A binary response (numeric) representing the treatment assignment\n",
    "            (coded as 0/1).\n",
    "        method: A sklearn model specifying which classification or regression\n",
    "            model to use. This should be a method that can handle a \n",
    "            multinominal class variable.\n",
    "\n",
    "    Return:\n",
    "        Dictionary: A dictionary of two models. One for the treatment group, \n",
    "            one for the control group.\n",
    "\n",
    "            {\n",
    "                'model_treat': a model for the treatment group,\n",
    "                'model_control': a model for the control group\n",
    "            }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    treat_rows = (t == 1)\n",
    "    control_rows = (t == 0)\n",
    "    model_treat = method(**kwargs).fit(x[treat_rows], y[treat_rows])\n",
    "    model_control = method(**kwargs).fit(x[control_rows], y[control_rows])\n",
    "    \n",
    "    res = {\n",
    "        'model_treat': model_treat,\n",
    "        'model_control': model_control,\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "def predict_tma(obj, newdata, **kwargs):\n",
    "    \"\"\"Predictions according to the \"Two Model Approach\" \n",
    "    (a.k.a. \"Separate Model Approach\")\n",
    "    \n",
    "    For each instance in newdata two predictions are made:\n",
    "    1) What is the probability of a person responding when treated?\n",
    "    2) What is the probability of a person responding when not treated\n",
    "      (i.e. part of control group)?\n",
    "\n",
    "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
    "\n",
    "    Args:\n",
    "        obj: A dictionary of two models. \n",
    "            One for the treatment group, one for the control group.\n",
    "        newdata: A data frame containing the values at which predictions\n",
    "            are required.\n",
    "    \n",
    "    Return:\n",
    "        DataFrame: A dataframe with predicted returns for when the customers\n",
    "            are treated and for when they are not treated.\n",
    "    \"\"\"\n",
    "   \n",
    "    if isinstance(obj['model_treat'], LinearRegression):\n",
    "        pred_treat = obj['model_treat'].predict(newdata)\n",
    "    else:\n",
    "        pred_treat = obj['model_treat'].predict_proba(newdata)[:, 1]\n",
    "\n",
    "    if isinstance(obj['model_control'], LinearRegression):\n",
    "        pred_control = obj['model_control'].predict(newdata)\n",
    "    else:\n",
    "        pred_control = obj['model_control'].predict_proba(newdata)[:, 1]\n",
    "    \n",
    "    # pred_treat = obj['model_treat'].predict(newdata)\n",
    "    # pred_control = obj['model_control'].predict(newdata)\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"pr_y1_t1\": pred_treat,\n",
    "        \"pr_y1_t0\": pred_control,\n",
    "    })\n",
    "    return pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "hREl_CRv9DYC",
    "outputId": "e5847999-b923-4de1-87eb-650813cf65bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def ty_assign(y, t):\n",
    "    if y == 1 and t == 1:\n",
    "        return \"TR\"\n",
    "    elif y == 0 and t == 1:\n",
    "        return \"TN\"\n",
    "    elif y == 1 and t == 0:\n",
    "        return \"CR\"\n",
    "    elif y == 0 and t == 0:\n",
    "        return \"CN\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def t_assign(ty):\n",
    "    if ty in (\"TR\", \"TN\"):\n",
    "        return 1\n",
    "    elif ty in (\"CR\", \"CN\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def y_assign(ty):\n",
    "    if ty in (\"TR\", \"CR\"):\n",
    "        return 1\n",
    "    elif ty in (\"TN\", \"CN\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "search_space = {\n",
    "    'method': [LogisticRegression],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'penalty': ['none', 'l2'],\n",
    "    'tol': [1e-2, 1e-3, 1e-4],\n",
    "    'C': [1e6, 1e3, 1, 1e-3, 1e-6],\n",
    "}\n",
    "\n",
    "def main():\n",
    "    ### Load data ###\n",
    "    df = pd.read_csv('Hillstrom.csv')\n",
    "    dataset = 'hillstrom'\n",
    "    df = preprocess_data(df)\n",
    "    Y = df['Y']\n",
    "    T = df['T']\n",
    "    X = df.drop(['Y', 'T'], axis=1)\n",
    "    ty = pd.DataFrame({'Y': Y, 'T': T})\\\n",
    "             .apply(lambda row: ty_assign(row['Y'], row['T']), axis=1)\n",
    "    if dataset == 'hillstrom':\n",
    "        fold_gen = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234).split(X, ty)\n",
    "    elif dataset == 'lalonde':\n",
    "        fold_gen = KFold(n_splits=5, shuffle=True, random_state=1234).split(X)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#     for model in models:\n",
    "\n",
    "    ### Cross validation ###\n",
    "    qini_list = []\n",
    "    for idx, (train_index, test_index) in enumerate(fold_gen):\n",
    "        X_train = X.reindex(train_index)\n",
    "        X_test = X.reindex(test_index)\n",
    "#         if dataset == 'hillstrom':\n",
    "#             Y = ty.apply(y_assign)\n",
    "#             T = ty.apply(t_assign)\n",
    "        Y_train = Y.reindex(train_index)\n",
    "        Y_test = Y.reindex(test_index)\n",
    "        T_train = T.reindex(train_index)\n",
    "        T_test = T.reindex(test_index)\n",
    "\n",
    "        df = X_train.copy()\n",
    "        df['Y'] = Y_train\n",
    "        df['T'] = T_train\n",
    "        stratify = T_train\n",
    "        if dataset == 'hillstrom':\n",
    "            stratify = df[['Y', 'T']]\n",
    "        tuning_df, validate_df = train_test_split(\n",
    "            df, test_size=0.33, random_state=1234, stratify=stratify)\n",
    "\n",
    "        X_tuning = tuning_df.drop(['Y', 'T'], axis=1)\n",
    "        Y_tuning = tuning_df['Y']\n",
    "        T_tuning = tuning_df['T']\n",
    "\n",
    "        X_validate = validate_df.drop(['Y', 'T'], axis=1)\n",
    "        Y_validate = validate_df['Y']\n",
    "        T_validate = validate_df['T']\n",
    "        \n",
    "        data_dict = {\n",
    "            \"x_train\": X_tuning,\n",
    "            \"y_train\": Y_tuning,\n",
    "            \"t_train\": T_tuning,\n",
    "            \"x_test\": X_validate,\n",
    "            \"y_test\": Y_validate,\n",
    "            \"t_test\": T_validate,\n",
    "        }\n",
    "        \n",
    "        model_method = search_space.get('method', None)\n",
    "        params = {\n",
    "            'method': None if model_method is None else model_method[0],\n",
    "        }\n",
    "        if params['method'] == LogisticRegression:\n",
    "            solver = search_space.get('solver', None)\n",
    "            params['solver'] = None if solver is None else solver[0]\n",
    "\n",
    "        _, drop_vars, qini_values = wrapper(\n",
    "                tma, predict_tma, data_dict, params=params)\n",
    "        best_qini = max(qini_values)\n",
    "        best_idx = qini_values.index(best_qini)\n",
    "        best_drop_vars = drop_vars[:best_idx]\n",
    "\n",
    "        X_tuning.drop(best_drop_vars, axis=1, inplace=True)\n",
    "        X_validate.drop(best_drop_vars, axis=1, inplace=True)\n",
    "        X_train.drop(best_drop_vars, axis=1, inplace=True)\n",
    "        X_test.drop(best_drop_vars, axis=1, inplace=True)\n",
    "\n",
    "        _, best_params = parameter_tuning(tma, predict_tma, data_dict, \n",
    "                                          search_space=search_space)\n",
    "\n",
    "        mdl = tma(X_train, Y_train, T_train, **best_params)\n",
    "        pred = predict_tma(mdl, X_test)\n",
    "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], Y_test, T_test)\n",
    "        q = qini(perf)\n",
    "        qini_list.append(q['qini'])\n",
    "    print('Qini values: ', qini_list)\n",
    "    print('    mean: {}, std: {}'.format(np.mean(qini_list), np.std(qini_list)))\n",
    "        \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9X42GUaj-UX"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, attribute, threshold):\n",
    "        self.attr = attribute\n",
    "        self.thres = threshold\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf = False\n",
    "        self.predict = None\n",
    "\n",
    "\n",
    "def select_threshold(df, attribute, predict_attr):\n",
    "    \"\"\"\n",
    "    Select the threshold of the attribute to split\n",
    "    The threshold chosen splits the test data such that information gain is maximized\n",
    "    \"\"\"\n",
    "    # Convert dataframe column to a list and round each value\n",
    "    values = df[attribute].tolist()\n",
    "    values = [float(x) for x in values]\n",
    "    # Remove duplicate values by converting the list to a set, then sort the set\n",
    "    values = set(values)\n",
    "    values = list(values)\n",
    "    values.sort()\n",
    "    max_ig = float(\"-inf\")\n",
    "    thres_val = 0\n",
    "    # try all threshold values that are half-way between successive values in this sorted list\n",
    "    for i in range(0, len(values) - 1):\n",
    "        thres = (values[i] + values[i+1])/2\n",
    "        ig = info_gain(df, attribute, predict_attr, thres)\n",
    "        if ig > max_ig:\n",
    "            max_ig = ig\n",
    "            thres_val = thres\n",
    "    # Return the threshold value that maximizes information gained\n",
    "    return thres_val\n",
    "\n",
    "\n",
    "def info_entropy(df, predict_attr):\n",
    "    \"\"\"\n",
    "    Calculate info content (entropy) of the test data\n",
    "    \"\"\"\n",
    "    # Dataframe and number of positive/negatives examples in the data\n",
    "    p_df = df[df[predict_attr] == 1]\n",
    "    n_df = df[df[predict_attr] == 0]\n",
    "    p = float(p_df.shape[0])\n",
    "    n = float(n_df.shape[0])\n",
    "    # Calculate entropy\n",
    "    if p  == 0 or n == 0:\n",
    "        I = 0\n",
    "    else:\n",
    "        I = ((-1*p)/(p + n))*math.log(p/(p+n), 2) + ((-1*n)/(p + n))*math.log(n/(p+n), 2)\n",
    "    return I\n",
    "\n",
    "\n",
    "def remainder(df, df_subsets, predict_attr):\n",
    "    \"\"\"\n",
    "    Calculates the weighted average of the entropy after an attribute test\n",
    "    \"\"\"\n",
    "    # number of test data\n",
    "    num_data = df.shape[0]\n",
    "    remainder = float(0)\n",
    "    for df_sub in df_subsets:\n",
    "        if df_sub.shape[0] > 1:\n",
    "            remainder += float(df_sub.shape[0]/num_data)*info_entropy(df_sub, predict_attr)\n",
    "    return remainder\n",
    "\n",
    "\n",
    "def info_gain(df, attribute, predict_attr, threshold):\n",
    "    \"\"\"\n",
    "    Calculates the information gain from the attribute test based on a given threshold\n",
    "    Note: thresholds can change for the same attribute over time\n",
    "    \"\"\"\n",
    "    sub_1 = df[df[attribute] <= threshold]\n",
    "    sub_2 = df[df[attribute] > threshold]\n",
    "    # Determine information content, and subract remainder of attributes from it\n",
    "    ig = info_entropy(df, predict_attr) - remainder(df, [sub_1, sub_2], predict_attr)\n",
    "    return ig\n",
    "\n",
    "\n",
    "def num_class(df, predict_attr):\n",
    "    \"\"\"\n",
    "    Returns the number of positive and negative data\n",
    "    \"\"\"\n",
    "    p_df = df[df[predict_attr] == 1]\n",
    "    n_df = df[df[predict_attr] == 0]\n",
    "    return p_df.shape[0], n_df.shape[0]\n",
    "\n",
    "\n",
    "def choose_attr(df, attributes, predict_attr):\n",
    "    \"\"\"\n",
    "    Chooses the attribute and its threshold with the highest info gain\n",
    "    from the set of attributes\n",
    "    \"\"\"\n",
    "    max_info_gain = float(\"-inf\")\n",
    "    best_attr = None\n",
    "    threshold = 0\n",
    "    # Test each attribute (note attributes maybe be chosen more than once)\n",
    "    for attr in attributes:\n",
    "        thres = select_threshold(df, attr, predict_attr)\n",
    "        ig = info_gain(df, attr, predict_attr, thres)\n",
    "        if ig > max_info_gain:\n",
    "            max_info_gain = ig\n",
    "            best_attr = attr\n",
    "            threshold = thres\n",
    "    return best_attr, threshold\n",
    "\n",
    "\n",
    "\n",
    "def build_tree(df, cols, predict_attr):\n",
    "    \"\"\"\n",
    "    Builds the Decision Tree based on training data, attributes to train on,\n",
    "    and a prediction attribute\n",
    "    \"\"\"\n",
    "    # Get the number of positive and negative examples in the training data\n",
    "    p, n = num_class(df, predict_attr)\n",
    "    # If train data has all positive or all negative values, or the number of\n",
    "    # the train data is less than the given minimum number of split, then we\n",
    "    # have reached the end of our tree\n",
    "    if p > 0 and n > 0 and (p+n) >= 100:\n",
    "        # Determine attribute and its threshold value with the highest\n",
    "        # information gain\n",
    "        best_attr, threshold = choose_attr(df, cols, predict_attr)\n",
    "        # Create internal tree node based on attribute and it's threshold\n",
    "        sub_1 = df[df[best_attr] <= threshold]\n",
    "        sub_2 = df[df[best_attr] > threshold]\n",
    "        if sub_1.shape[0] > 0 and sub_1.shape[1] > 0:\n",
    "            tree = Node(best_attr, threshold)\n",
    "            # Recursively build left and right subtree\n",
    "            tree.left = build_tree(sub_1, cols, predict_attr)\n",
    "            tree.right = build_tree(sub_2, cols, predict_attr)\n",
    "            return tree\n",
    "    # Create a leaf node indicating it's prediction\n",
    "    leaf = Node(None,None)\n",
    "    leaf.leaf = True\n",
    "    leaf.predict = p / (p+n)\n",
    "    return leaf\n",
    "\n",
    "\n",
    "def predict(node, row_df):\n",
    "    \"\"\"\n",
    "    Given a instance of a training data, make a prediction of an observation (row)\n",
    "    based on the Decision Tree\n",
    "    Assumes all data has been cleaned (i.e. no NULL data)\n",
    "    \"\"\"\n",
    "    # If we are at a leaf node, return the prediction of the leaf node\n",
    "    if node.leaf:\n",
    "        return node.predict\n",
    "    # Traverse left or right subtree based on instance's data\n",
    "    if row_df[node.attr] <= node.thres:\n",
    "        return predict(node.left, row_df)\n",
    "    elif row_df[node.attr] > node.thres:\n",
    "        return predict(node.right, row_df)\n",
    "\n",
    "\n",
    "def test_predictions(root, df, target_attr='y'):\n",
    "    \"\"\"\n",
    "    Given a set of data, make a prediction for each instance using the Decision Tree\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "    for index,row in df.iterrows():\n",
    "        prediction.append(predict(root, row))\n",
    "    pred_df = pd.Series(prediction)\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "\n",
    "# An example use of 'build_tree' and 'predict'\n",
    "df_train = X_train.copy()\n",
    "df_train['Y'] = Y_train\n",
    "df_train['T'] = T_train\n",
    "df_test = X_test.copy()\n",
    "df_test['Y'] = Y_test\n",
    "df_test['T'] = T_test\n",
    "\n",
    "df_train.drop(['history'], axis=1, inplace=True)\n",
    "df_test.drop(['history'], axis=1, inplace=True)\n",
    "\n",
    "assert((df_train.columns == df_test.columns).all())\n",
    "attributes = [c for c in df_train.columns if c != 'Y']\n",
    "root = build_tree(df_train, attributes, 'Y')\n",
    "pred = test_predictions(root, df_test, target_attr='Y')\n",
    "print('pred: {}'.format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "skeleton.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
