{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehi1150I7fxN"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_hilstrom = 'https://drive.google.com/open?id=15osyN4c5z1pSo1JkxwL_N8bZTksRvQuU'\n",
    "fluff, id = link.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('Hillstrom.csv')\n",
    "hillstrom_df = pd.read_csv('Hillstrom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tAS92JMPe9U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link_ = 'https://drive.google.com/open?id=1b8N7WtwIe2WmQJD1KL5UAy70K13MxwKj'\n",
    "fluff, id = link.split('=')\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('Lalonde.csv')\n",
    "lalonde_df = pd.read_csv('Lalonde.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "ZoNrZI5P80wJ",
    "outputId": "88345ba3-d1bc-47e4-c3d7-0686b1bed600"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "def preprocess_data(df, dataset='hillstrom', verbose=True):\n",
    "    # For Hillstrom dataset, the ‘‘visit’’ target variable was selected\n",
    "    #   as the target variable of interest and the selected treatment is \n",
    "    #   the e-mail campaign for women’s merchandise [1]\n",
    "    # [1] Kane K, Lo VSY, Zheng J. True-lift modeling: Comparison of methods. \n",
    "    #    J Market Anal. 2014;2:218–238\n",
    "    dataset = dataset.lower()\n",
    "    if dataset in ('hillstrom', 'email'):\n",
    "        columns = df.columns\n",
    "        for col in columns:\n",
    "            if df[col].dtype != object:\n",
    "                continue\n",
    "            df = pd.concat(\n",
    "                    [df, pd.get_dummies(df[col], \n",
    "                                        prefix=col, \n",
    "                                        drop_first=False)],\n",
    "                    axis=1)\n",
    "            df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        df.columns = [col.replace('-', '').replace(' ', '_').lower()\n",
    "                      for col in df.columns]\n",
    "        df = df[df.segment_mens_email == 0]\n",
    "        df.index = range(len(df))\n",
    "        df.drop(['segment_mens_email', \n",
    "                 'segment_no_email', \n",
    "                 'conversion', \n",
    "                 'spend'], axis=1, inplace=True)\n",
    "\n",
    "        y_name = 'visit'\n",
    "        t_name = 'segment_womens_email'\n",
    "    elif dataset in ['criteo', 'ad']:\n",
    "        raise NotImplementedError\n",
    "    elif dataset in ['lalonde', 'job']:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    df['Y'] = df[y_name]\n",
    "    df.drop([y_name], axis=1, inplace=True)\n",
    "    df['T'] = df[t_name]\n",
    "    df.drop([t_name], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Igf3QLgdJ1cW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def performance(pr_y1_t1, pr_y1_t0, y, t, groups=10):\n",
    "    \"\"\"\n",
    "    1. Split the total customers into the given number of groups\n",
    "    2. Calculate the statistics of each segment\n",
    "    \n",
    "    Args:\n",
    "        pr_y1_t1: the series (list) of the customer's expected return\n",
    "        pr_y1_t0: the expected return when a customer is not treated\n",
    "        y: the observed return of customers\n",
    "        t: whther each customer is treated or not\n",
    "        groups: the number of groups (segments). Should be 5, 10, or 20\n",
    "    Return:\n",
    "        DataFrame:\n",
    "            columns:\n",
    "                'n_y1_t1': the number of treated responders\n",
    "                'n_y1_t0': the number of not treated responders\n",
    "                'r_y1_t1': the average return of treated customers\n",
    "                'r_y1_t0': the average return of not treated customers\n",
    "                'n_t1': the number of treated customers\n",
    "                'n_t0': the number of not treated customers\n",
    "                'uplift': the average uplift (the average treatment effect)\n",
    "            rows: the index of groups\n",
    "    \"\"\"\n",
    "  \n",
    "    ### check valid arguments\n",
    "    if groups not in [5, 10, 20]:\n",
    "        raise Exception(\"uplift: groups must be either 5, 10 or 20\")\n",
    "  \n",
    "    ### check for NAs.\n",
    "    if pr_y1_t1.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in pr_y1_t1\")\n",
    "    if pr_y1_t0.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in pr_y1_t0\")\n",
    "    if y.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in y\")\n",
    "    if t.isnull().values.any():\n",
    "        raise Exception(\"uplift: NA not permitted in t\")\n",
    "   \n",
    "    ### check valid values for y and t\n",
    "    # if set(y) != {0, 1}:\n",
    "    #     raise Exception(\"uplift: y must be either 0 or 1\")\n",
    "    if set(t) != {0, 1}:\n",
    "        raise Exception(\"uplift: t must be either 0 or 1\")\n",
    "\n",
    "    ### check length of arguments\n",
    "    if not (len(pr_y1_t1) == len(pr_y1_t0) == len(y) == len(t)):\n",
    "        raise Exception(\"uplift: arguments pr_y1_t1, pr_y1_t0, y and t must all have the same length\")\n",
    "\n",
    "    ### define dif_pred\n",
    "    dif_pred = pr_y1_t1 - pr_y1_t0\n",
    "  \n",
    "    ### Make index same\n",
    "    y.index = dif_pred.index\n",
    "    t.index = dif_pred.index\n",
    "    \n",
    "    mm = pd.DataFrame({\n",
    "        'dif_pred': dif_pred,\n",
    "        'y': y,\n",
    "        't': t,\n",
    "        'dif_pred_r': dif_pred.rank(ascending=False, method='first')\n",
    "    })\n",
    "\n",
    "    mm_groupby = mm.groupby(pd.qcut(mm['dif_pred_r'], groups, labels=range(1, groups+1), duplicates='drop'))\n",
    "  \n",
    "    n_y1_t1 = mm_groupby.apply(lambda r: r[r['t'] == 1]['y'].sum())\n",
    "    n_y1_t0 = mm_groupby.apply(lambda r: r[r['t'] == 0]['y'].sum())\n",
    "    n_t1 = mm_groupby['t'].sum()\n",
    "    n_t0 = mm_groupby['t'].count() - n_t1\n",
    "  \n",
    "    df = pd.DataFrame({\n",
    "        'n_t1': n_t1,\n",
    "        'n_t0': n_t0,\n",
    "        'n_y1_t1': n_y1_t1,\n",
    "        'n_y1_t0': n_y1_t0,\n",
    "        'r_y1_t1': n_y1_t1 / n_t1,\n",
    "        'r_y1_t0': n_y1_t0 / n_t0,\n",
    "    })\n",
    "    fillna_columns = ['n_y1_t1', 'n_y1_t0', 'n_t1', 'n_t0']\n",
    "    df[fillna_columns] = df[fillna_columns].fillna(0)\n",
    "    df.index.name = 'groups'\n",
    "\n",
    "    df['uplift'] = df['r_y1_t1'] - df['r_y1_t0']\n",
    "    df['uplift'] = round(df['uplift'], 6)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def qini(perf, plotit=True):\n",
    "    nrow = len(perf)\n",
    "\n",
    "    # Calculating the incremental gains. \n",
    "    # - First, the cumulitative sum of the treated and the control groups are\n",
    "    #  calculated with respect to the total population in each group at the\n",
    "    #  specified decile\n",
    "    # - Afterwards we calculate the percentage of the total amount of people\n",
    "    #  (both treatment and control) are present in each decile\n",
    "    cumul_y1_t1 = (perf['n_y1_t1'].cumsum() / perf['n_t1'].cumsum()).fillna(0)\n",
    "    cumul_y1_t0 = (perf['n_y1_t0'].cumsum() / perf['n_t0'].cumsum()).fillna(0)\n",
    "    deciles = [i/nrow for i in range(1, nrow+1)]\n",
    "\n",
    "    ### Model Incremental gains\n",
    "    inc_gains = (cumul_y1_t1 - cumul_y1_t0) * deciles\n",
    "    inc_gains = [0.0] + list(inc_gains)\n",
    "\n",
    "    ### Overall incremental gains\n",
    "    overall_inc_gain = sum(perf['n_y1_t1']) / sum(perf['n_t1']) \\\n",
    "            - sum(perf['n_y1_t0']) / sum(perf['n_t0'])\n",
    "\n",
    "    ### Random incremental gains\n",
    "    random_inc_gains = [i*overall_inc_gain / nrow for i in range(nrow+1)]\n",
    "\n",
    "    ### Compute area under the model incremental gains (uplift) curve\n",
    "    x = [0] + deciles\n",
    "    y = list(inc_gains)\n",
    "    auuc = 0\n",
    "    auuc_rand = 0\n",
    "\n",
    "    auuc_list = [auuc]\n",
    "    for i in range(1, len(x)):\n",
    "        auuc += 0.5 * (x[i] - x[i-1]) * (y[i] + y[i-1])\n",
    "        auuc_list.append(auuc)\n",
    "\n",
    "    ### Compute area under the random incremental gains curve\n",
    "    y_rand = random_inc_gains\n",
    "\n",
    "    auuc_rand_list = [auuc_rand]\n",
    "    for i in range(1, len(x)):\n",
    "        auuc_rand += 0.5 * (x[i] - x[i-1]) * (y_rand[i] + y_rand[i-1])\n",
    "        auuc_rand_list.append(auuc_rand)\n",
    "\n",
    "    ### Compute the difference between the areas (Qini coefficient)\n",
    "    Qini = auuc - auuc_rand\n",
    "\n",
    "    ### Plot incremental gains curve\n",
    "    if plotit:\n",
    "        x_axis = x\n",
    "        plt.plot(x_axis, inc_gains)\n",
    "        plt.plot(x_axis, random_inc_gains)\n",
    "        plt.show()\n",
    "    \n",
    "    ### Qini 30%, Qini 10%\n",
    "    n_30p = int(nrow*3/10)\n",
    "    n_10p = int(nrow/10)\n",
    "    qini_30p = auuc_list[n_30p] - auuc_rand_list[n_30p]\n",
    "    qini_10p = auuc_list[n_10p] - auuc_rand_list[n_10p]\n",
    "\n",
    "    res = {\n",
    "        'qini': Qini,\n",
    "        'inc_gains': inc_gains,\n",
    "        'random_inc_gains': random_inc_gains,\n",
    "        'auuc_list': auuc_list,\n",
    "        'auuc_rand_list': auuc_rand_list,\n",
    "        'qini_30p': qini_30p,\n",
    "        'qini_10p': qini_10p,\n",
    "    }    \n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nrxjl1v7J9Mm"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def parameter_tuning(fit_mdl, pred_mdl, data, search_space):\n",
    "    \"\"\"\n",
    "    Given a model, search all combination of parameter sets and find\n",
    "    the best parameter set\n",
    "    \n",
    "    Args:\n",
    "        fit_mdl: model function\n",
    "        pred_mdl: predict function of fit_mdl\n",
    "        data:\n",
    "            {\n",
    "                \"x_train\": predictor variables of training dataset,\n",
    "                \"y_train\": target variables of training dataset,\n",
    "                \"t_train\": treatment variables of training dataset,\n",
    "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
    "                \"y_test\": target variables of test (usually, validation) dataset,\n",
    "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
    "            }\n",
    "        search_space:\n",
    "            {\n",
    "                parameter_name: [search values]\n",
    "            }\n",
    "    Return:\n",
    "        The best parameter set\n",
    "    \"\"\"\n",
    "    \n",
    "    ###############################\n",
    "    ###     Do it yourself!     ###\n",
    "    ###############################\n",
    "    return ##\n",
    "\n",
    "  \n",
    "def wrapper(fit_mdl, pred_mdl, data):\n",
    "    \"\"\"\n",
    "    General wrapper approach\n",
    "    \n",
    "    Args:\n",
    "        fit_mdl: model function\n",
    "        pred_mdl: predict function of fit_mdl\n",
    "        data:\n",
    "            {\n",
    "                \"x_train\": predictor variables of training dataset,\n",
    "                \"y_train\": target variables of training dataset,\n",
    "                \"t_train\": treatment variables of training dataset,\n",
    "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
    "                \"y_test\": target variables of test (usually, validation) dataset,\n",
    "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
    "            }\n",
    "    Return:\n",
    "        (A list of best models, The list of dropped variables)\n",
    "    \"\"\"\n",
    "    \n",
    "    ###############################\n",
    "    ###     Do it yourself!     ###\n",
    "    ###############################\n",
    "    return ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxrEbEODlbQi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def tma(x, y, t, method=LogisticRegression, **kwargs):\n",
    "    \"\"\"Training a model according to the \"Two Model Approach\" \n",
    "    (a.k.a. \"Separate Model Approach\")\n",
    "    The default model is General Linear Model (GLM)\n",
    "    \n",
    "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
    "\n",
    "    Args:\n",
    "        x: A data frame of predictors.\n",
    "        y: A binary response (numeric) vector.\n",
    "        t: A binary response (numeric) representing the treatment assignment\n",
    "            (coded as 0/1).\n",
    "        method: A sklearn model specifying which classification or regression\n",
    "            model to use. This should be a method that can handle a \n",
    "            multinominal class variable.\n",
    "\n",
    "    Return:\n",
    "        Dictionary: A dictionary of two models. One for the treatment group, \n",
    "            one for the control group.\n",
    "\n",
    "            {\n",
    "                'model_treat': a model for the treatment group,\n",
    "                'model_control': a model for the control group\n",
    "            }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    treat_rows = (t == 1)\n",
    "    control_rows = (t == 0)\n",
    "    model_treat = method(**kwargs).fit(x[treat_rows], y[treat_rows])\n",
    "    model_control = method(**kwargs).fit(x[control_rows], y[control_rows])\n",
    "    \n",
    "    res = {\n",
    "        'model_treat': model_treat,\n",
    "        'model_control': model_control,\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "def predict_tma(obj, newdata, **kwargs):\n",
    "    \"\"\"Predictions according to the \"Two Model Approach\" \n",
    "    (a.k.a. \"Separate Model Approach\")\n",
    "    \n",
    "    For each instance in newdata two predictions are made:\n",
    "    1) What is the probability of a person responding when treated?\n",
    "    2) What is the probability of a person responding when not treated\n",
    "      (i.e. part of control group)?\n",
    "\n",
    "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
    "\n",
    "    Args:\n",
    "        obj: A dictionary of two models. \n",
    "            One for the treatment group, one for the control group.\n",
    "        newdata: A data frame containing the values at which predictions\n",
    "            are required.\n",
    "    \n",
    "    Return:\n",
    "        DataFrame: A dataframe with predicted returns for when the customers\n",
    "            are treated and for when they are not treated.\n",
    "    \"\"\"\n",
    "   \n",
    "    if isinstance(obj['model_treat'], LinearRegression):\n",
    "        pred_treat = obj['model_treat'].predict(newdata)\n",
    "    else:\n",
    "        pred_treat = obj['model_treat'].predict_proba(newdata)[:, 1]\n",
    "\n",
    "    if isinstance(obj['model_control'], LinearRegression):\n",
    "        pred_control = obj['model_control'].predict(newdata)\n",
    "    else:\n",
    "        pred_control = obj['model_control'].predict_proba(newdata)[:, 1]\n",
    "    \n",
    "    # pred_treat = obj['model_treat'].predict(newdata)\n",
    "    # pred_control = obj['model_control'].predict(newdata)\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"pr_y1_t1\": pred_treat,\n",
    "        \"pr_y1_t0\": pred_control,\n",
    "    })\n",
    "    return pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "hREl_CRv9DYC",
    "outputId": "e5847999-b923-4de1-87eb-650813cf65bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def ty_assign(y, t):\n",
    "    if y == 1 and t == 1:\n",
    "        return \"TR\"\n",
    "    elif y == 0 and t == 1:\n",
    "        return \"TN\"\n",
    "    elif y == 1 and t == 0:\n",
    "        return \"CR\"\n",
    "    elif y == 0 and t == 0:\n",
    "        return \"CN\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def t_assign(ty):\n",
    "    if ty in (\"TR\", \"TN\"):\n",
    "        return 1\n",
    "    elif ty in (\"CR\", \"CN\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def y_assign(ty):\n",
    "    if ty in (\"TR\", \"CR\"):\n",
    "        return 1\n",
    "    elif ty in (\"TN\", \"CN\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    ### Load data ###\n",
    "    df = pd.read_csv('Hillstrom.csv')\n",
    "    dataset = 'hillstrom'\n",
    "    df = preprocess_data(df)\n",
    "    Y = df['Y']\n",
    "    T = df['T']\n",
    "    X = df.drop(['Y', 'T'], axis=1)\n",
    "    ty = pd.DataFrame({'Y': Y, 'T': T})\\\n",
    "             .apply(lambda row: ty_assign(row['Y'], row['T']), axis=1)\n",
    "    if dataset == 'hillstrom':\n",
    "        fold_gen = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234).split(X, ty)\n",
    "    elif dataset == 'lalonde':\n",
    "        fold_gen = KFold(n_splits=5, shuffle=True, random_state=1234).split(X)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#     for model in models:\n",
    "\n",
    "    ### Cross validation ###\n",
    "    for idx, (train_index, test_index) in enumerate(fold_gen):\n",
    "        X_train = X.reindex(train_index)\n",
    "        X_test = X.reindex(test_index)\n",
    "        if dataset == 'hillstrom':\n",
    "            Y = ty.apply(y_assign)\n",
    "            T = ty.apply(t_assign)\n",
    "        Y_train = Y.reindex(train_index)\n",
    "        Y_test = Y.reindex(test_index)\n",
    "        T_train = T.reindex(train_index)\n",
    "        T_test = T.reindex(test_index)\n",
    "\n",
    "        mdl = tma(X_train, Y_train, T_train)\n",
    "        pred = predict_tma(mdl, X_test)\n",
    "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], Y_test, T_test)\n",
    "        q = qini(perf)\n",
    "        \n",
    "#         print(\"Model: {}\\n\".format(model))\n",
    "#         print(\"Tuning space: \\n\")\n",
    "#         for key, val in search_space.items():\n",
    "#             print(\"    '{}': {}\\n\".format(key, val))\n",
    "#         print(\"Seed: {}\\n\".format(seed))\n",
    "#         print(\"Qini value: mean = {}, std = {}\\n\\n\".format(mean_qini, std_qini))\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9X42GUaj-UX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "skeleton.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
